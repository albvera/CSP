Two of the most successful data-structures enabling fast shortest path queries at scale are \emph{contraction hierarchies} (CH)~\citep{geisberger_ch_definition} and \emph{hub labels} (HL)~\citep{cohen_definition_hl}.
These are general techniques which always guarantee correct SP computation, but have no uniform storage/query-time bounds for all graphs. 
We now explain the construction for HL; for the construction and results of CH refer to Appendix~\ref{sec:ch}.

The basic HL technique for SP computations is as follows:
Every node $v$ is associated with a set of forward hubs $\Lf(v)\subseteq V$ and reverse hubs $\Lb(v)\subseteq V$.
We also store $\dist(v,w) \,\forall\, w\in\Lf(v)$ and $\dist(u,v)\,\forall\,u\in\Lb(v)$.
The hub labels are said to satisfy the \emph{cover property} if, for any $s\neq t\in V$, $\Lf(s)\cap\Lb(t)$ contains at least one node in $P(s,t)$.
In the case that $t$ is not reachable from $s$, it must be that $\Lf(s)\cap\Lb(t)=\varnothing$.

With the aid of the cover property, we can obtain $\dist(s,t)$ by searching for the minimum value of $\dist(s,w)+\dist(w,t)$ over all nodes $w\in\Lf(s)\cap\Lb(t)$.
If the hubs are sorted by ID, this can be done in time $\Or(\card{\Lf(s)}+\card{\Lb(t)})$ via a single sweep.
Moreover, by storing the second node in $P(s,w)$ for each $w\in \Lf(s)$, and the penultimate node in $P(w,t)$ for each $w\in \Lb(t)$, we can also recover the shortest path recursively, as each HL query returns at least one new node $w\in P(s,t)$.
Note that we need to store this extra information, otherwise we could have $\Lf(s)\cap\Lb(t)=\crl{s}$.
Let $\Lm \defeq \max_v\crl{\abs*{\Lf(v)}+\abs*{\Lb(v)}}$ be the size of the maximum HL.
The per-node storage requirement is $\Or(\Lm)$, while the query time is $\Or(\Lm\ell(P(s,t)))$.

Although hub labels always exist (in particular, we can always choose $\Lf(s)$ to be the set of nodes reachable from $v$, and $\Lb(s)$ the set of nodes that can reach $v$), finding \emph{optimal} hub-labels (in terms of storage/query-time bounds) is known to be NP-hard~\citep{babenko_hl_complexity}.
To construct hub labels with guarantees on preprocessing time and $\Lm$, we need the additional notion of a \emph{multi-scale LSHS}. 
We assume that the graph $(G,\ell)$ admits a collection of sets $\crl*{C_i: i=1,\ldots,\log D}$, such that each $C_i$ is an $(h,2^{i-1})$-LSHS.
Given such a collection, we can now obtain small HL.
We outline this construction for directed graphs, closely following the construction in \cite[Theorem 5.1]{highway2013} for the undirected case.
\begin{proposition}
\label[proposition]{theo:construct_hl}
For $(G,\ell)$, given a multi-scale LSHS collection $\crl*{C_i:i=0,\ldots,\log D}$, where each $C_i$ is an $(h,2^{i-1})$-LSHS, we can construct hub labels of size at most $h(1+\log D)$.
\end{proposition}
\begin{proof}
For each node $v$, we define the hub label $L(v)$ as

\begin{equation*}
\Lf(v)\defeq  \bigcup_{i=0}^{\log D}C_i\cap \Bf_{2^i}(v) \quad\text{ and }\quad
\Lb(v)\defeq \bigcup_{i=0}^{\log D}C_i\cap \Bb_{2^i}(v).
\end{equation*}

Since each $C_i$ is an $(h,2^{i-1})$-LSHS which we intersect with balls of radius $2\cdot 2^{i-1}$, every set in the union contributes at most $h$ elements and the maximum size is as claimed.

To prove the cover property, we note that, if $t$ is not reachable from $s$, by definition $\Lf(s)\cap\Lb(t)=\varnothing$.
This is because the elements in $\Lf(s)$ are reachable from $s$ and the elements in $\Lb(t)$ reach $t$.
On the other hand, when $P(s,t)$ exists, let $i$ be such that $2^{i-1}<\ell(P(s,t))\leq 2^i$.
Finally, any point in the path belongs to both $\Bf_{2^i}(s)$ and $\Bb_{2^i}(t)$, and hence $C_i\cap P(s,t)$ is in both hubs (which is not empty since $C_i$ hits all SP of length $\geq 2^{i-1}$).
\end{proof}


Finally, we need to compute the desired multi-scale LSHS in polynomial time.
In \cref{sec:preproc} we show that, if the $\hd\leq h$, in polynomial time we can obtain sparsity $h'=\Or(h\Delta\log(h\Delta))$.
In other words, if we are not given the multi-scale LSHS and have to compute them in polynomial time, the HL have size $h'(1+\log D)$ instead of $h(1+\log D)$.
A more subtle point is that the resulting algorithm, even though polynomial, is impractical for large networks.
In \cref{sec:numeric}, we discuss heuristics that work better in practice.