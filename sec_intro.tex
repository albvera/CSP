Motivated by the requirements of modern transportation systems, we consider the fast computation of constrained shortest paths (CSP) in large-scale graphs. 
Though the unconstrained shortest-path (SP) problem has a long history, it has been revolutionized by recent algorithmic advancements that help enable large-scale mapping applications (cf.~\citep{goldberg_survey,dimacs09} for surveys).
In particular, the use of preprocessing techniques and network augmentation has led to dramatic improvements in the scalability of SP computation for road networks.
These techniques, however, do not extend to the CSP, and hence can not fully leverage the rich travel-time distribution data available today.

The SP and CSP problems can be summarized as follows: 
We are given a graph $G$, where each edge has an associated \emph{length} and \emph{cost}. 
The SP problem requires finding an $(s,t)$-path of minimum length for any given nodes $s$ and $t$. 
The CSP problem inputs an additional budget $b$, and requires finding a minimum length $(s,t)$-path \emph{with total cost at most $b$}.
The two problems, though similar, have very different runtime complexity: SP queries admit polynomial-time algorithms (in particular, the famous Dijkstra's algorithm), while CSP computation is known to be NP-Hard~\citep{csp_survey}.
That said, a standard dynamic program computes CSPs in pseudo-polynomial time for discrete costs~\citep{alex_bicriteria}, and gives a natural scaling-based FPTAS for continuous costs (as in the knapsack problem).

Though there is a rich literature on CSP \citep{csp_survey}, existing approaches do not scale to support modern applications. 
To this end, we study \emph{preprocessing and network augmentation for speeding up CSP computation}.
Our work contributes to a growing field of algorithms for large-scale problems (non-convex methods, sketching techniques, etc.), with relatively poor worst-case performance, but which are provably efficient for practically relevant settings.

\paragraph{Applications of large-scale CSP computation:}
Our primary motivation for scaling CSP comes from the requirements of modern transportation platforms (Lyft, Uber etc.) for accurate routing and travel-time estimates.
Modern SP engines like Google Maps do not make full use of available traffic information.
In particular, they do not incorporate uncertainties in travel times, leading to inaccurate estimates in settings with high traffic variability.
This can be addressed by computing shortest paths based on \emph{probabilistic} travel-time estimates:
if $\Pst$ denotes the set of $(s,t)$-paths and $\ell$ the (possibly random) length of an edge or path, given $s,t$ and parameters $p,\delta$, we want to find an $P\in\Pst$ minimizing $\E[\ell(P)]$, subject to $\Pr(\ell(P)>\E[\ell(P)]+\delta)\leq p$.
Computing this exactly for general distributions is expensive due to the need for computing convolutions of distributions -- indeed, there is no known polynomial time algorithm for doing this even with black-box access to convolution solvers~\citep{nikolova_gaussian}. 
However, conditioning on public state variables (e.g. weather and traffic in key neighborhoods),  we can approximate the distributions with uncorrelated travel-times across different road segments~\citep{woodard2017predicting}. 
Thus, Chebyshev's inequality gives us that $\Pr(\sum_e\ell_e>\E[\sum_e\ell_e]+\delta)\leq \sum_e\Var(\ell_e)/\delta^2$. 
Using this, we can reformulate the stochastic path optimization problem as 
$\min_{ P\in\Pst}\sum_{i\in P}\mu_i \, \mbox{s.t.}\, \sum_{i\in P}\sigma^2_i\leq \delta^2p$, which is now a CSP problem (see also \citep{nikolova_gaussian} for a similar approach for Gaussian travel times). 
Note that, though we relax the condition $\Pr(\ell(P)>\E[\ell(P)]+\delta)\leq p$, our solution always respects this constraint -- this is often critical for practical applications, e.g., for Lyft/Uber, accuracy of ETA estimates is more important than choosing the optimal route.

Another problem that can be modelled as a CSP is that of finding \emph{reliable shortest paths}.
Consider the case where each edge has a probability $q_e$ of triggering a bad event, with resulting penalty $p$ (for example, slowdowns due to accidents).
In this case, we want to minimize the travel time as well as the expected penalty.
Assuming independence, we have the following natural problem:
$\min_{P\in\Pst} \ell(P)+p\prn*{1-\prod_{e\in P}(1-q_e)}$.
This model is considered in \citep{fareevasion} for routing with fare evasion, where $q_e$ is the probability of encountering an inspector, and $p$ the penalty; the authors suggest using a CSP formulation, wherein the non-linear objective is replaced by a linear constraint by taking logarithms.


\subsection{Our Contributions}
We consider the problem of developing oracles that support fast CSP queries in large networks. 
Specifically, given integer edge-costs and edge-lengths, and a budget upper bound, we want to use preprocessing to create a data-structure supporting arbitrary source-destination-budget queries; moreover, we want formal guarantees on the performance: preprocessing, storage and query time.

In the case of SP algorithms, the seminal work of Abraham et al.~\citep{highway2013, highway2010} demonstrated that the preprocessing, storage and query times of several widely-used heuristics could be parametrized using a graph structural metric called the \emph{Highway Dimension} (HD).
Our work extends these notions for the CSP; our contributions are summarized as follows:

\paragraph{Theoretical contributions}: 
We define the \emph{constrained highway dimension} (CHD) for the set of {\em efficient paths} (i.e., minimal solutions to the CSP; cf. \cref{def:effpath}). 
We show how the CHD can be used to parametrize the performance of CSP algorithms.
One hurdle, however, is that the CHD can be much bigger than the HD in general.
Our main theoretical contribution is in showing that the \emph{HD and CHD an be related under an additional partial witness condition} (\cref{def:partial_witness}); therefore, in settings where SP computation is scalable, we can also solve the harder problem of CSP. 
Our next contribution is to show that, under \emph{average performance metrics}, we can obtain even weaker conditions to relate the \emph{average CHD and HD}, thus certifying the performance of our algorithms.
The conditions can be interpreted in terms of having few physical overpasses in road networks.

\paragraph{Practical contributions}: 
We use our theoretical results to develop new practical data-structures for CSP queries, based on {\em hub labels}~\citep{cohen_definition_hl}. 
We evaluate our algorithm on datasets with detailed travel-time information for San Francisco and Luxembourg.
In experiments, our algorithms exhibit query-times $10^4$ faster than existing (non-preprocessing) techniques, have small storage requirements, and good preprocessing times even on a single machine. 
 

\paragraph{Paper outline}:
In \cref{sec:prelim}, we introduce the SP and CSP problems, and extend the notion of the HD (as defined in \citep{highway2013}) to directed graphs and general path systems; this allows us to define an analogous notion of a \emph{constrained highway dimension} (CHD) for constrained shortest paths in \cref{sec:chd}. 
We then show that the two can be related under an additional \emph{partial witness condition} (\cref{ssec:hdvschd}). 
In \cref{sec:avg_hd}, we study average-case performance, and show how a small average CHD can be related to physical overpasses in road networks. 
Finally, in \cref{sec:numeric}, we present our practical hub-label construction and our experiments on SF and Luxembourg data.


\subsection{Related work}

CSP problems have an extensive literature, surveyed in~\citep{csp_survey}. 
More recently, there has been significant interest in stochastic SP problems, including the related stochastic on-time arrival (SOTA) problem~\citep{fan2005arriving}; recent works have proposed both optimal and approximate policies~\citep{sabran2014precomputation,nikolova_discretization}. 
Existing approaches for these problems, however, are limited in their use of preprocessing and augmentation techniques, and consequently do not support the latencies required for mapping applications.

As we mention before, our work is inspired by the recent developments in shortest path algorithms~\citep{highway2013,hubimplem,highway2010,dimacs09,geisberger_ch_definition,skeleton}; refer~\citep{goldberg_survey} for an excellent survey of these developments. 
The pre-processing technique we use for speeding up CSP computations is hub labels (HL), first introduced for SP computations in~\citep{cohen_definition_hl}. 
More recently, HL was proved to have the best query-time bounds for SP computation in low HD graphs~\citep{highway2013,highway2010} (this was experimentally confirmed in~\citep{hubimplem}, \cite[Figure 7]{goldberg_survey}).  
Finally, the HD-based bounds for hub labels was shown to be tight in~\citep{babenko_hl_complexity,white_complexity_hd}, and it was also shown that finding optimal hub labels is NP hard.

Finally, a related class of problems to CSP is that of SP under label constraints~\citep{language_csp}, where the aim is to find shortest paths that avoided certain labels (e.g. toll roads, ferries, etc.). 
In this setting, there is work on using preprocessing to improve query-times~\citep{rice_csp}.
These problems are essentially concatenations of parallel SP problems, involving only local constraints. 
In contrast, the CSP involves global constraints on paths.
Our results do in fact shed light on why preprocessing works well for label-constrained SP queries.