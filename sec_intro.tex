Motivated by the requirements of modern transportation systems, we consider the fast computation of constrained shortest paths (CSP) in large-scale graphs. Though the basic shortest-path (SP) problem has a long history, it has been revolutionized by recent algorithmic advancements that help enable large-scale mapping applications (cf.~\cite{goldberg_survey,dimacs09} for surveys).
In particular, the use of preprocessing techniques and network augmentation has led to dramatic improvements in the scalability of SP computation for road networks.
These techniques however do not extend to the CSP, hence can not fully leverage the rich travel-time distribution data available today.

The SP and CSP problems can be summarized as follows: We are given a graph $G$, where each edge has an associated \emph{length} and \emph{cost}. 
The SP problem requires finding an $(s,t)$-path of minimum length for any given nodes $s$ and $t$. 
The CSP problem inputs an additional budget $b$, and requires finding a minimum length $(s,t)$-path \emph{with total cost less than $b$}.
The two problems, though similar, have very different runtime complexity: SP queries admit polynomial-time algorithms (in particular, the famous Dijkstra's algorithm), while CSP computation is known to be NP-Hard~\cite{csp_survey}.
That said, a standard dynamic program computes CSPs in pseudo-polynomial time for discrete costs~\cite{alex_bicriteria}, and gives a natural scaling-based FPTAS for continuous costs (as in the knapsack problem).

Though there is a rich literature on CSP (cf. \cite{csp_survey}), existing approaches do not scale to support modern applications. 
To this end, we study \emph{preprocessing and network augmentation for speeding up CSP computation}.
Our work contributes to a growing field of algorithms for large-scale problems (non-convex methods, sketching techniques, etc.), with poor worst-case performance, but which are provably efficient for practically relevant settings.


\paragraph{Applications of large-scale CSP computation}
Our primary motivation for scaling CSP comes from the requirements of modern transportation platforms (Lyft, Uber, Waze etc.) for accurate routing and travel-time estimates.
Modern SP engines like Google Maps and OSRM do not make full use of available traffic information.
In particular, they do not incorporate uncertainties in travel times, leading to inaccurate estimates in settings with high traffic variability.
This can be addressed by computing shortest paths based on \emph{robust} travel-time estimates:
given $s,t$ and parameters $p,\delta$, we want to find an $(s,t)$-path $P$ minimizing $\E[\ell(P)]$, subject to $\Pr(\ell(P)>\E[\ell(P)]+\delta)\leq p$.
Computing this exactly for general distributions is expensive due to the need for computing convolutions of distributions. 
However, if we condition on state variables (e.g. weather and traffic in key neighborhoods),  we can approximate the distributions with uncorrelated travel-times across different road segments~\cite{woodard2017predicting}. 
Thus, Chebyshev's inequality gives us that $\Pr(\sum_eT_e>\E[\sum_eT_e]+\delta)\leq \sum_e\Var(T_e)/\delta^2$. Using this, we can reformulate the robust trip-time estimation problem as 
$\min_{ P\in\Pst}\sum_{i\in P}\mu_i \, \mbox{s.t.}\, \sum_{i\in P}\sigma^2_i\leq \delta^2p$, which is now a CSP problem. 
Note that though we relax the condition $\Pr(\ell(P)>\E[\ell(P)]+\delta)\leq p$, our solution always respects this constraint -- this is often more critical for practical applications, e.g., for ETA estimates accuracy is more important than optimality.

Another problem that can be modelled as a CSP is that of finding \emph{reliable shortest paths}.
Consider the case where each edge has a probability $q_e$ of triggering a bad event, with resulting penalty $p$ (for example, slowdowns due to accidents).
In this case, we want to minimize the travel time as well as the expected penalty.
Assuming independence, we have the following natural problem:
$\min_{P\in\Pst} \ell(P)+p\prn*{1-\prod_{e\in P}(1-q_e)}$.
This model is considered in \cite{fareevasion} for routing with fare evasion, where $q_e$ is the probability of encountering an inspector, and $p$ the penalty; the authors suggest using a CSP formulation, wherein the non-linear objective is replaced by a linear constraint by taking logarithms.


\subsection{Our Contributions}
We consider the problem of developing oracles that support fast CSP queries in large networks. 
In particular, given a network with integer edge-costs and edge-lengths, and a budget upper bound, we want to preprocess the network to create a data-structure that supports arbitrary source-destination-budget queries; moreover, we want formal guarantees on the preprocessing time, storage and query time.
 In the context of the SP problem, the seminal work of Abraham et al.~\cite{highway2013, highway2010} demonstrated that the preprocessing, storage and query times of several widely-used heuristics~\cite{dimacs09} could be parametrized using a graph structural metric called the \emph{Highway Dimension} (HD).
Our work adopts a similar program for the CSP; in particular, our contributions are summarized as follows:

\paragraph{Theoretical contributions}: 
Analogous to the role of the Highway Dimension for the set of shortest paths, we define the \emph{constrained highway dimension} (CHD) for the set of {\em efficient paths} (i.e., minimal solutions to the CSP; cf. Defn.~\ref{def:effpath}). 
We show how the CHD can be used to parametrize the performance of CSP algorithms via a reduction from CSP to an SP problem on a directed \emph{augmented graph}.
One hurdle however is that the CHD can be much bigger than the HD in general; our main theoretical contribution is in showing that the \emph{HD and CHD an be related under an additional partial witness condition} (\cref{def:partial_witness}). 
This justifies our proposed CSP algorithms, as it shows they are efficient in settings where SP computation is scalable. Moreover, we show that under \emph{average performance metrics} (as opposed to HD which is a worst-case notion), we can obtain an explicit condition for small gaps between the average HD and CHD, which can be interpreted in terms of having few physical overpasses in road networks.

\paragraph{Practical contributions}: 
We use our theoretical results to develop new practical data-structures for CSP queries, based on {\em hub labels}~\cite{cohen_definition_hl}. 
We evaluate our algorithm on datasets with detailed travel-time information for San Francisco and Luxembourg.
In experiments, our algorithms exhibit query times four orders of magnitude faster than existing (non-preprocessing) techniques, have small storage requirements, and good preprocessing times even on a single machine. 
 

\paragraph{Paper outline}:
In \cref{sec:prelim}, we introduce the SP and CSP problems, and extend the notion of the HD (as defined in \cite{highway2013}) to directed graphs and general path systems; this allows us to define an analogous notion of a \emph{constrained highway dimension} (CHD) for constrained shortest paths in \cref{sec:chd}. 
We then show that the two can be related under an additional \emph{partial witness condition} (\cref{ssec:hdvschd}). 
In \cref{sec:avg_hd}, we study average-case performance, and show how a small average CHD can be related to physical overpasses in road networks. 
Finally, in \cref{sec:numeric}, we present our practical hub-label construction and our experiments on SF and Luxembourg data.


\subsection{Related work}

CSP problems have an extensive literature, surveyed in~\cite{csp_survey}. 
More recently, there has been significant interest in robust SP problems, as well as the related stochastic on-time arrival (SOTA) problem~\cite{fan2005arriving}; recent works have proposed both optimal and approximate policies~\cite{sabran2014precomputation,nikolova_discretization}. 
Existing approaches for these problems, however, are limited in their use of preprocessing and augmentation techniques, and consequently do not support the latencies required for mapping applications.

As we mention before, our work is inspired by the recent developments in shortest path algorithms~\cite{highway2013,hubimplem,highway2010,dimacs09,geisberger_ch_definition,skeleton}; refer~\cite{goldberg_survey} for an excellent survey of these developments. 
The pre-processing technique we use for speeding up CSP computations is hub labels (HL), first introduced for SP computations in~\cite{cohen_definition_hl}. 
More recently, HL was proved to have the best query-time bounds for SP computation in low HD graphs~\cite{highway2013,highway2010} (this was experimentally confirmed in~\cite{hubimplem}, \cite[Figure 7]{goldberg_survey}).  
Finally, the HD-based bounds for hub labels was shown to be tight in~\cite{babenko_hl_complexity,white_complexity_hd}, and it was also shown that finding optimal hub labels is NP hard.

Finally, a related class of problems to CSP is that of SP under label constraints~\cite{language_csp}, where the aim is to find shortest paths that avoided certain labels (e.g. toll roads, ferries, etc.). 
In this setting, there is work on using preprocessing to improve query-times~\cite{rice_csp}.
These problems are essentially concatenations of parallel SP problems, involving only local constraints. 
In contrast, the CSP involves global constraints on paths.
Our results do in fact shed light on why preprocessing works well for label-constrained SP queries.