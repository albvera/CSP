%\documentclass[opre,blindrev]{informs3}
\documentclass[opre,nonblindrev]{informs3} % current default for manuscript submission

%\DoubleSpacedXI % Made default 4/4/2014 at request
%%\OneAndAHalfSpacedXI % current default line spacing
%%\OneAndAHalfSpacedXII
%%\DoubleSpacedXII

% If hyperref is used, dvi-to-ps driver of choice must be declared as
%   an additional option to the \documentclass. For example
%\documentclass[dvips,opre]{informs3}      % if dvips is used
%\documentclass[dvipsone,opre]{informs3}   % if dvipsone is used, etc.

%%% OPRE uses endnotes. If you do not use them, put a percent sign before
%%% the \theendnotes command. This template does show how to use them.
%\usepackage{endnotes}
%\let\footnote=\endnote
%\let\enotesize=\normalsize
%\def\notesname{Endnotes}%
%\def\makeenmark{$^{\theenmark}$}
%\def\enoteformat{\rightskip0pt\leftskip0pt\parindent=1.75em
%  \leavevmode\llap{\theenmark.\enskip}}

% Private macros here (check that there is no clash with the style)
\usepackage[colorlinks]{hyperref}
\usepackage{commandscsp}
\usepackage[capitalize]{cleveref}

\renewenvironment{proof}[1][\textsc{Proof.}]{#1 }{\hfill $\Box$} 

% Natbib setup for author-year style
\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%

%% Setup of theorem styles. Outcomment only one.
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
\ECRepeatTheorems

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

% In the reviewing and copyediting stage enter the manuscript number.
%\MANUSCRIPTNO{} % When the article is logged in and DOI assigned to it,
                 %   this manuscript number is no longer necessary


\begin{document}

% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% \RUNAUTHOR{Jones, Miller, and Wilson}
% Enter authors following the given pattern:
\RUNAUTHOR{Vera, Banerjee and Samaranayake}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
\RUNTITLE{Computing Constrained Shortest-Paths at Scale}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\TITLE{Computing Constrained Shortest-Paths at Scale}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
\ARTICLEAUTHORS{%
\AUTHOR{Alberto Vera, Siddhartha Banerjee, Samitha Samaranayake}
\AFF{Cornell University} %, \URL{}}
} % end of the block

\ABSTRACT{%
Motivated by the needs of modern transportation service platforms, we study the problem of computing constrained shortest paths (CSP) at scale via preprocessing techniques.
Our work makes two contributions in this regard:\\
1. We propose a scalable algorithm for CSP queries, and show how its performance can be parametrized in terms of a new network primitive, the \emph{constrained highway dimension}.
This development extends recent work which established the \emph{highway dimension} as the appropriate primitive for characterizing the performance of unconstrained shortest-path (SP) algorithms.
Our main theoretical contribution is deriving conditions relating the two notions, thereby providing a characterization of networks where CSP and SP queries are of comparable hardness.\\
2. We develop practical algorithms for scalable CSP computation, augmenting our theory with additional network clustering heuristics. 
We evaluate these algorithms on real-world datasets to validate our theoretical findings. 
Our techniques are orders of magnitude faster than existing approaches, while requiring only limited additional storage and preprocessing.
}%

\KEYWORDS{Constrained Shortest Path; Distance Oracles; Highway Dimension.}


\maketitle

\section{Introduction}
Motivated by the requirements of modern transportation systems, we consider the fast computation of constrained shortest paths (CSP) in large-scale graphs. 
Though the unconstrained shortest-path (SP) problem has a long history, it has been revolutionized by recent algorithmic advancements that help enable large-scale mapping applications (cf.~\citep{goldberg_survey,dimacs09} for surveys).
In particular, the use of preprocessing techniques and network augmentation has led to dramatic improvements in the scalability of SP computation for road networks.
These techniques, however, do not extend to the CSP, and hence can not fully leverage the rich travel-time distribution data available today.

The SP and CSP problems can be summarized as follows: 
We are given a graph $G$, where each edge has an associated \emph{length} and \emph{cost}. 
The SP problem requires finding an $(s,t)$-path of minimum length for any given nodes $s$ and $t$. 
The CSP problem inputs an additional budget $b$, and requires finding a minimum length $(s,t)$-path \emph{with total cost at most $b$}.
The two problems, though similar, have very different runtime complexity: SP queries admit polynomial-time algorithms (in particular, the famous Dijkstra's algorithm), while CSP computation is known to be NP-Hard~\citep{csp_survey}.
That said, a standard dynamic program computes CSPs in pseudo-polynomial time for discrete costs~\citep{alex_bicriteria}, and gives a natural scaling-based FPTAS for continuous costs (as in the knapsack problem).

Though there is a rich literature on CSP \citep{csp_survey}, existing approaches do not scale to support modern applications. 
To this end, we study \emph{preprocessing and network augmentation for speeding up CSP computation}.
Our work contributes to a growing field of algorithms for large-scale problems (non-convex methods, sketching techniques, etc.), with relatively poor worst-case performance, but which are provably efficient for practically relevant settings.

\paragraph{Applications of large-scale CSP computation:}
Our primary motivation for scaling CSP comes from the requirements of modern transportation platforms (Lyft, Uber etc.) for accurate routing and travel-time estimates.
Modern SP engines like Google Maps do not make full use of available traffic information.
In particular, they do not incorporate uncertainties in travel times, leading to inaccurate estimates in settings with high traffic variability.
This can be addressed by computing shortest paths based on \emph{probabilistic} travel-time estimates:
if $\Pst$ denotes the set of $(s,t)$-paths and $\ell$ the (possibly random) length of an edge or path, given $s,t$ and parameters $p,\delta$, we want to find an $P\in\Pst$ minimizing $\E[\ell(P)]$, subject to $\Pr(\ell(P)>\E[\ell(P)]+\delta)\leq p$.
Computing this exactly for general distributions is expensive due to the need for computing convolutions of distributions -- indeed, there is no known polynomial time algorithm for doing this even with black-box access to convolution solvers~\cite{nikolova_gaussian}. 
However, conditioning on public state variables (e.g. weather and traffic in key neighborhoods),  we can approximate the distributions with uncorrelated travel-times across different road segments~\citep{woodard2017predicting}. 
Thus, Chebyshev's inequality gives us that $\Pr(\sum_e\ell_e>\E[\sum_e\ell_e]+\delta)\leq \sum_e\Var(\ell_e)/\delta^2$. 
Using this, we can reformulate the stochastic path optimization problem as 
$\min_{ P\in\Pst}\sum_{i\in P}\mu_i \, \mbox{s.t.}\, \sum_{i\in P}\sigma^2_i\leq \delta^2p$, which is now a CSP problem (see also \cite{nikolova_gaussian} for a similar approach for Gaussian travel times). 
Note that, though we relax the condition $\Pr(\ell(P)>\E[\ell(P)]+\delta)\leq p$, our solution always respects this constraint -- this is often critical for practical applications, e.g., for Lyft/Uber, accuracy of ETA estimates is more important than choosing the optimal route.

Another problem that can be modelled as a CSP is that of finding \emph{reliable shortest paths}.
Consider the case where each edge has a probability $q_e$ of triggering a bad event, with resulting penalty $p$ (for example, slowdowns due to accidents).
In this case, we want to minimize the travel time as well as the expected penalty.
Assuming independence, we have the following natural problem:
$\min_{P\in\Pst} \ell(P)+p\prn*{1-\prod_{e\in P}(1-q_e)}$.
This model is considered in \citep{fareevasion} for routing with fare evasion, where $q_e$ is the probability of encountering an inspector, and $p$ the penalty; the authors suggest using a CSP formulation, wherein the non-linear objective is replaced by a linear constraint by taking logarithms.


\subsection{Our Contributions}
We consider the problem of developing oracles that support fast CSP queries in large networks. 
Specifically, given integer edge-costs and edge-lengths, and a budget upper bound, we want to use preprocessing to create a data-structure supporting arbitrary source-destination-budget queries; moreover, we want formal guarantees on the performance: preprocessing, storage and query time.

In the case of SP algorithms, the seminal work of Abraham et al.~\citep{highway2013, highway2010} demonstrated that the preprocessing, storage and query times of several widely-used heuristics could be parametrized using a graph structural metric called the \emph{Highway Dimension} (HD).
Our work extends these notions for the CSP; our contributions are summarized as follows:

\paragraph{Theoretical contributions}: 
We define the \emph{constrained highway dimension} (CHD) for the set of {\em efficient paths} (i.e., minimal solutions to the CSP; cf. \cref{def:effpath}). 
We show how the CHD can be used to parametrize the performance of CSP algorithms.
One hurdle, however, is that the CHD can be much bigger than the HD in general.
Our main theoretical contribution is in showing that the \emph{HD and CHD an be related under an additional partial witness condition} (\cref{def:partial_witness}); therefore, in settings where SP computation is scalable, we can also solve the harder problem of CSP. 
Our next contribution is to show that, under \emph{average performance metrics}, we can obtain even weaker conditions to relate the \emph{average CHD and HD}, thus certifying the performance of our algorithms.
The conditions can be interpreted in terms of having few physical overpasses in road networks.

\paragraph{Practical contributions}: 
We use our theoretical results to develop new practical data-structures for CSP queries, based on {\em hub labels}~\citep{cohen_definition_hl}. 
We evaluate our algorithm on datasets with detailed travel-time information for San Francisco and Luxembourg.
In experiments, our algorithms exhibit query-times $10^4$ faster than existing (non-preprocessing) techniques, have small storage requirements, and good preprocessing times even on a single machine. 
 

\paragraph{Paper outline}:
In \cref{sec:prelim}, we introduce the SP and CSP problems, and extend the notion of the HD (as defined in \citep{highway2013}) to directed graphs and general path systems; this allows us to define an analogous notion of a \emph{constrained highway dimension} (CHD) for constrained shortest paths in \cref{sec:chd}. 
We then show that the two can be related under an additional \emph{partial witness condition} (\cref{ssec:hdvschd}). 
In \cref{sec:avg_hd}, we study average-case performance, and show how a small average CHD can be related to physical overpasses in road networks. 
Finally, in \cref{sec:numeric}, we present our practical hub-label construction and our experiments on SF and Luxembourg data.


\subsection{Related work}

CSP problems have an extensive literature, surveyed in~\citep{csp_survey}. 
More recently, there has been significant interest in stochastic SP problems, including the related stochastic on-time arrival (SOTA) problem~\citep{fan2005arriving}; recent works have proposed both optimal and approximate policies~\citep{sabran2014precomputation,nikolova_discretization}. 
Existing approaches for these problems, however, are limited in their use of preprocessing and augmentation techniques, and consequently do not support the latencies required for mapping applications.

As we mention before, our work is inspired by the recent developments in shortest path algorithms~\citep{highway2013,hubimplem,highway2010,dimacs09,geisberger_ch_definition,skeleton}; refer~\citep{goldberg_survey} for an excellent survey of these developments. 
The pre-processing technique we use for speeding up CSP computations is hub labels (HL), first introduced for SP computations in~\citep{cohen_definition_hl}. 
More recently, HL was proved to have the best query-time bounds for SP computation in low HD graphs~\citep{highway2013,highway2010} (this was experimentally confirmed in~\citep{hubimplem}, \cite[Figure 7]{goldberg_survey}).  
Finally, the HD-based bounds for hub labels was shown to be tight in~\citep{babenko_hl_complexity,white_complexity_hd}, and it was also shown that finding optimal hub labels is NP hard.

Finally, a related class of problems to CSP is that of SP under label constraints~\citep{language_csp}, where the aim is to find shortest paths that avoided certain labels (e.g. toll roads, ferries, etc.). 
In this setting, there is work on using preprocessing to improve query-times~\citep{rice_csp}.
These problems are essentially concatenations of parallel SP problems, involving only local constraints. 
In contrast, the CSP involves global constraints on paths.
Our results do in fact shed light on why preprocessing works well for label-constrained SP queries.

\section{Preliminaries}
\label{sec:prelim}
\label{ssec:basic}
We consider a directed graph $G=(V,E)$, where each edge $e\in E$ has an associated \emph{length} $\ell(e)\in\N_+$, and \emph{cost} $c(e)\in\N_+\cup\crl{0}$.
For each node $v$, we denote its degree $\Delta(v)$ as the sum of the in-degree and out-degree, and define the \emph{maximum degree} $\Delta \defeq \max_v\Delta(v)$.
For any source-terminal pair $s,t\in V$, we denote by $\Pst$ the set of all simple $(s,t)$-paths (without loops or cycles). 
Throughout this work, we only consider simple paths, which we refer to as paths for brevity.

For any path $P$, we define its length $\ell(P)$ and cost $c(P)$ as the sum of edge lengths and edge costs in $P$. 
Note that any path $P$ with more than one node has length at least $1$ (since we assume lengths are integers).
For $s,t\in V$, the distance from $s$ to $t$, denoted $\dist(s,t)$, is the smallest length among all paths $P\in\Pst$.
For a node $v$ and a path $P$, we abuse notation to denote $\dist(v,P)$ as the minimum distance from $v$ to any node $w\in P$; the distance $\dist(P,v)$ from $P$ to $v$ is defined analogously.
Note that  $\dist(P,v)$ and  $\dist(v,P)$ need not be the same as the graph is directed.
We denote the shortest $(s,t)$-path (if it exists) as $P(s,t)$, and denote the set of all shortest paths in $G$ as $\PS$.
Finally, we define $D\defeq\max_{P\in\PS}\ell(P)$ to be the diameter of $G$.

Our goal is to develop a data-structure to answer \emph{Constrained Shortest-Path} (CSP) queries efficiently: 
Given a source-terminal pair $s,t$ and a budget $b$, we want to return a path $P$ solving
\[
\min\crl{\ell(P):c(P) \leq b, P \in \Pst }.
\]
We define $\dist(s,t|b)$ to be the minimum of this problem.
If there is no feasible solution, we define $\dist(s,t|b)=\infty$.
Note that the CSP problem may have multiple solutions as there could be several paths with the same length and cost lower than $b$.
To limit these solutions to those with minimal cost, we require that the path also be \emph{efficient}. 
\begin{definition}[Efficient Path]
\label[definition]{def:effpath}
A path $P\in \Pst$ is called \emph{efficient} if there is no other path $P'\in \Pst$ such that $\ell(P')\leq \ell(P)$ and $c(P')\leq c(P)$ with at least one inequality strict.
\end{definition}
We denote the set of all efficient paths as $\PE$.
Efficient $(s,t)$-paths form what is known as the \emph{Pareto frontier}, $\Pst\cap\PE$.
Observe that every subpath of an efficient path is also efficient (if not, we could improve the path by replacing the subpath).
For $r>0$ and $v\in V$, we define the \emph{forward and reverse balls of radius $r$} by $\Bf_r(v)\defeq\{u\in V: \dist(v,u)\leq r\}$ and $\Bb_r(v)\defeq\{u\in V: \dist(u,v)\leq r\}$, and also define $B_r(v)\defeq\Bf_r(v)\cup\Bb_r(v)$.
Finally, a graph $G$ is said to have a \emph{doubling dimension} $\alpha$ if, for any node $v$ and any $r>0$, the ball $B_{2r}(v)$ can be covered by at most $\alpha$ balls of radius $r$.

\subsection{Hitting sets and the highway dimension}
\label{ssec:hddef}
We now define some additional network primitives, which we need to parametrize the performance of our CSP algorithms. 
In particular, we use the \emph{highway dimension}, introduced by \citep{highway2013,highway2010} to parametrize shortest-path computations in undirected graphs. 
A few of our results are generalizations of those in \citep{highway2013}; the technical challenges of these extensions may not be clear to a non-expert reader, thus we differ all discussions on the matter to \cref{app:generalhd}. 
Very broadly speaking, \cite{highway2013} deals only with undirected graphs and shortest paths, whereas our approach covers directed graphs and general sets of paths.

We define a \emph{path system} $\calQ$ as any collection of paths.
We say that a set $C\subseteq V$ \emph{hits} any given path $Q$ if some node in $Q$ belongs to $C$. Moreover, we say that $C$ is a \emph{hitting set for a path system} $\calQ$ if it hits every $Q\in\calQ$. 
For any $r>0$, we say a path $Q$ is $r$-significant if $\ell(Q)>r$. 
For a given path system $\calQ$, we denote $\calQ_r$ as the set of all $r$-significant paths in $\calQ$.
Hitting sets are useful for compressing path systems. 
In particular, even if the hitting set is large, the extent to which a path system can be compressed depends on the \emph{local sparsity} of hitting sets with respect to \emph{significant paths} of $\calQ$.

\begin{definition}[Locally-Sparse Hitting Sets]\label[definition]{def:lshs}
Given a path system $\calQ$ and $r>0$, an $(h,r)$ locally-sparse hitting set (or $(h,r)$-LSHS) is a set $C\subseteq V$ with two properties: 
\begin{enumerate}[nosep]
\item Hitting: $C$ is a hitting set for $\calQ_r$.
\item Local sparsity: for every $v\in V$, $\abs*{B_{2r}(v)\cap C}\leq h$.
\end{enumerate}
\end{definition}


As we discuss in \cref{ssec:hldef}, the existence of $(h,r)$-LSHS immediately enables the compression of path system $\calQ$ via the construction of \emph{hub labels}. 
However, the existence of LSHS does not guarantee the ability to efficiently compute these objects. 
To address this, we need a stronger notion; the \emph{highway dimension} is a property that ensures both existence and efficient computation of LSHS.
To define the highway dimension (HD), we first need two additional definitions:
for $v\in V, r>0$, the \emph{forward path-neighbourhood} with respect to a path system $\calQ$ is $
S_r^+(v,\calQ) \defeq\crl*{Q\in\calQ_r: \dist(v,Q)\leq 2r}$ and similarly $S_r^-(v,\calQ) \defeq\crl*{Q\in\calQ_r: \dist(Q,v)\leq 2r}$ is the reverse neighbourhood.
As before, $S_r(v,\calQ) \defeq S_r^+(v,\calQ)\cup S_r^-(v,\calQ)$. 
Now we can define the HD of a path system $\calQ$. 
Essentially, the HD re-orders the sequence of qualifiers in the definition of $(h,r)$-LSHS: it requires the existence of a small hitting set for each individual neighborhood, rather than a single hitting set which is locally sparse. 
\begin{definition}[Highway Dimension]	\label[definition]{def:hddef}
	A path system $\calQ$ has HD $h$ if, $\forall r>0,v\in V$, there exists a set $H_{v,r}\subseteq V$ such that $\card{H_{v,r}}\leq h$ and $H_{v,r}$ is a hitting set for $S_r(v,\calQ)$.
\end{definition}

As shorthand, we refer to the HD of $(G,\ell)$ as that of $\PS$. 
Note that $HD\leq h$ is a more stringent requirement than the existence of an $(h,r)$-LSHS $C$, since $C\cap B_{2r}(v)$ need not hit all the paths in $S_r(v,\calQ)$. 
However, if $G$ has $HD \leq h$, then this guarantees the existence of a $(h,r)$-LSHS according to the following result, which can be proven by adapting the proof from \cite[Theorem 4.2]{highway2013} to our general case.
\begin{proposition}\label{prop:equiv}
If the path system $\calQ$ has HD $h$, then, $\forall\,r>0$, there exists an $(h,r)$-LSHS.
\end{proposition}

More importantly, note that the result is about existence and does not touch on computability.
As we discuss in Section~\ref{sec:preproc}, if $G$ has $HD \leq h$, then this permits efficient computation of LSHS.

\subsection{Shortest-Paths via Hub Labels}
\label{ssec:hldef}
Two of the most successful data-structures enabling fast shortest path queries at scale are \emph{contraction hierarchies} (CH)~\cite{geisberger_ch_definition} and \emph{hub labels} (HL)~\cite{cohen_definition_hl}.
These are general techniques which always guarantee correct SP computation, but have no uniform storage/query-time bounds for all graphs. 
We now explain the construction for HL; for the construction and results of CH refer to Appendix~\ref{sec:ch}.

The basic HL technique for SP computations is as follows:
Every node $v$ is associated with a hub label $L(v) = \crl*{\Lf(v),\Lb(v)}$, comprising of a set of forward hubs $\Lf(v)$ and reverse hubs $\Lb(v)$.
We also store $\dist(v,w) \,\forall\, w\in\Lf(v)$ and $\dist(u,v)\,\forall\,u\in\Lb(v)$.
The hub labels are said to satisfy the \emph{cover property} if, for any $s\neq t\in V$, $\Lf(s)\cap\Lb(t)$ contains at least one node in $P(s,t)$.
In the case that $t$ is not reachable from $s$, it must be that $\Lf(s)\cap\Lb(t)=\varnothing$.

With the aid of the cover property, we can obtain $\dist(s,t)$ by searching for the minimum value of $\dist(s,w)+\dist(w,t)$ over all nodes $w\in\Lf(s)\cap\Lb(t)$.
If the hubs are sorted by ID, this can be done in time $\Or(\card{\Lf(s)}+\card{\Lb(t)})$ via a single sweep.
Moreover, by storing the second node in $P(s,w)$ for each $w\in \Lf(s)$, and the penultimate node in $P(w,t)$ for each $w\in \Lb(t)$, we can also recover the shortest path recursively, as each HL query returns at least one new node $w\in P(s,t)$.
Note that we need to store this extra information, otherwise we could have $\Lf(s)\cap\Lb(t)=\crl{s}$.
Let $\Lm \defeq \max_v\abs*{\Lf(v)}+\max_v\abs*{\Lb(v)}$ be the size of the maximum HL.
The per-node storage requirement is $\Or(\Lm)$, while the query time is $\Or(\Lm\ell(P(s,t)))$.

Although hub labels always exist (in particular, we can always choose $\Lf(s)$ to be the set of nodes reachable from $v$, and $\Lb(s)$ the set of nodes that can reach $v$), finding \emph{optimal} hub-labels (in terms of storage/query-time bounds) is known to be NP-hard~\cite{babenko_hl_complexity}.
To construct hub labels with guarantees on preprocessing time and $\Lm$, we need the additional notion of a \emph{multi-scale LSHS}. 
We assume that graph $(G,\ell)$ admits a collection of sets $\crl*{C_i: i=1,\ldots,\log D}$, such that each $C_i$ is an $(h,2^{i-1})$-LSHS.
Given such a collection, we can now obtain small HL.
We outline this construction for directed graphs, closely following the construction in \cite[Theorem 5.1]{highway2013} for the undirected case.
\begin{proposition}
\label[proposition]{theo:construct_hl}
For $(G,\ell)$, given a multi-scale LSHS collection $\crl*{C_i:i=0,\ldots,\log D}$, where each $C_i$ is an $(h,2^{i-1})$-LSHS, we can construct hub labels of size at most $h(1+\log D)$.
\end{proposition}
\begin{proof}
For each node $v$, we define the hub label $L(v)$ as

\begin{equation*}
\Lf(v)\defeq  \bigcup_{i=0}^{\log D}C_i\cap \Bf_{2^i}(v) \quad\text{ and }\quad
\Lb(v)\defeq \bigcup_{i=0}^{\log D}C_i\cap \Bb_{2^i}(v).
\end{equation*}

Since each $C_i$ is an $(h,2^{i-1})$-LSHS which we intersect with balls of radius $2\cdot 2^{i-1}$, every set in the union contributes at most $h$ elements and the maximum size is as claimed.

To prove the cover property, we note that, if $t$ is not reachable from $s$, by definition $\Lf(s)\cap\Lb(t)=\varnothing$.
This is because the elements in $\Lf(s)$ are reachable from $s$ and the elements in $\Lb(t)$ reach $t$.
On the other hand, when $P(s,t)$ exists, let $i$ be such that $2^{i-1}<\ell(P(s,t))\leq 2^i$.
Finally, any point in the path belongs to both $\Bf_{2^i}(s)$ and $\Bb_{2^i}(t)$, and hence $C_i\cap P(s,t)$ is in both hubs (which is not empty since $C_i$ hits all SP of length $\geq 2^{i-1}$).
\end{proof}


Finally, we need to compute the desired multi-scale LSHS in polynomial time.
In \cref{sec:preproc} we show that, if the HD is $h$, in polynomial time we can obtain sparsity $h'=\Or(h\Delta\log(h\Delta))$.
In other words, the HL have size $h'(1+\log D)$ instead of $h(1+\log D)$ if we are not given the multi-scale LSHS and have to compute them in polynomial time.
A more subtle point is that the resulting algorithm, even though polynomial, is impractical for large networks.
In \cref{sec:numeric}, we discuss heuristics that work better in practice.

\section{Scalable CSP Algorithms: Theoretical Guarantees}
\label{sec:chd}
We develop a data-structure that supports fast queries for \emph{efficient paths}. 
Specifically, given a graph $G=(V,E)$ and a maximum budget $B$, we construct a data-structure such that, for any $s,t\in V$ and $b\leq B$, we return the length of the shortest $(s,t)$-path with cost at most $b$, denoted $\dist(s,t|b)$.
The actual path can easily be recovered as discussed in \cref{ssec:hldef}, hence we focus on querying $\dist(s,t|b)$ only.

In Section \ref{ssec:hldef} we discussed that, if a graph $G$ has HD $h$, we can simultaneously bound the preprocessing time, storage requirements and query time for constructing hub labels as functions of $h$.
This suggests that for the construction of provably efficient hub labels for the CSP problem, we need an analogous property for the set of \emph{efficient paths}.
\begin{definition}[Constrained Highway Dimension] The constrained highway dimension (CHD) of $(G,\ell,c)$, denoted $h_c$, is the HD of the efficient-path system $\PE$.
\end{definition}
Note that, since every shortest path is efficient, $h_c\geq h$.

We now have two main issues with this definition: first, it is unclear how this can be used to get hub labels, and second, it is unclear how the corresponding hub labels compare with those for shortest-path computations. 
To address this, we first convert efficient paths in $G$ to shortest paths in a larger \emph{augmented graph}. 
In Section \ref{ssec:hlcsp}, we use this to construct hub labels for CSP queries whose storage and query complexity can be bounded as $Bh_c$ (which can be strengthened further to $g(b)h_c$, where $g(b)$ measures the size of the Pareto frontier, cf. Section~\ref{sec:frontier}.). 
Finally, in Section \ref{ssec:hdvschd}, we show that the hub labels for CSP queries can in fact be related to the hub labels for SP queries under an additional natural condition on the efficient paths.

\subsection{Augmented Graph}
\label{ssec:aug}
In order to link the constrained highway dimension to hub labels, we first convert the original graph $G$ (with length and cost functions) into an \emph{augmented graph} $G^B$ with only edge lengths, such that the \emph{efficient paths of $G$ are in bijection with the shortest paths of $G^B$}.
We achieve this as follows: Each node in $G^B$ is of the form $\pp{v,b}$, which encodes the information of the remaining budget $b\geq 0$ and location $v\in V$.
A node is connected to neighbors (according to $E$) as long as the remaining budget of that transition is non-negative.
Finally, we create $n$ sink nodes, denoted $v^-$, and connect node $\pp{v,b}$ to $v^-$ with length $1/(b+1)$.
An illustration of the construction is presented in Figure~\ref{fig:augmented}.
The following definition formalizes this. 

\begin{definition}[Augmented Graph]
Given $(G,\ell,c)$ and $B\in\N$, the augmented version $G^B$ has vertex set $V^B \defeq\crl*{\pp{v,b}: v\in V, b=0,1,\ldots,B}\cup\crl*{v^-:v\in V}$, the edge set $E^B$ is\\ $\crl*{\pp{v,b}\pp{w,x} : vw\in E, x=b-c_{vw}, x\geq 0}\cup\crl*{\pp{v,b}v^-: v\in V, 0\leq b\leq B}$.
The length function in $G^B$ is $\ell(\pp{v,b},v^-) \defeq \frac{1}{b+1}$ and $\ell(\pp{v,b},\pp{w,x}) \defeq \ell(vv')$.
\end{definition}

\begin{figure}
\input{TexImg/augmented.tex}
\caption{Example of a graph augmentation: The original graph $G$ has all paths of unit length, and costs as indicated on the edges. In the augmented graph $G^B$, the labels represent the edge lengths (unlabeled edges have length 1). Note the additional edges from $(w,b)$ to the sink node $w^-$ (and similarly for $u$ and $v$). 
}
\label{fig:augmented}
\end{figure}

Paths in $G^B$ are mapped to paths in $G$ in the intuitive way, by removing the budget labels and sink nodes.
We call this mapping the \emph{projection} of a path.
%\begin{definition}
%Let $P$ be a path in $G^B$.
%If $P$ is of the form $\pp{v_1,b_1}\ldots\pp{v_k,b_k}$, then the projection of $P$, denoted $\bar P$, is the path $\bar P\defeq v_1v_2\ldots v_k$.
%Analogously, if $P$ is of the form $\pp{v_1,b_1}\ldots\pp{v_k,b_k}v_{k}^-$, then $\bar P\defeq v_1\ldots v_{k}$. 
%\end{definition}


\begin{proposition}\label{prop:shorteffic}
A shortest path from source $\pp{s,b}$ to sink node~$t^-$ projects to an efficient path in $G$ solving $\dist(s,t|b)$. 
\end{proposition}
\begin{proof}
Let $P$ be the shortest path from $\pp{s,b}$ to $t^-$, and $\bar P$ its projection.
To reach $t^-$, $P$ must pass through some $\pp{t,b'}$, $b'\geq 0$.
By construction, $P$ consumes $b-b'$ units of resource, hence it is feasible; moreover, $\bar P$ is the shortest among $(s,t)$-paths with cost $b-b'$.
Now assume, by way of contradiction, that $\bar P$ is not efficient.
As $\bar P$ is the shortest using $b-b'$ units of resource, there exists $P'$ such that $\ell(\bar P')\leq \ell(\bar P)$ and $c(\bar P')< c(\bar P)$.
It must be that $P'$ passes through $\pp{t,b''}$, with $b''>b'$.
We argue that, in this case, $P$ would not be a shortest path to $t^-$.
Indeed, 
\[
\ell(P')=\ell(\bar P')+\frac{1}{1+b''}
\leq \ell(\bar P) +\frac{1}{1+b''}
< \ell(\bar P) +\frac{1}{1+b'},
\]
where the last expression is exactly $\ell(P)$.
\end{proof}


Later we give a construction that depends on LSHS for the system $\PE$, we call this object an efficient path hitting set (EPHS).
The next result allows us to relate EPHS of the original graph to LSHS in the augmented graph.
Note that in $G^B$ we are interested only in shortest paths ending in sink nodes (since these map to efficient paths). 
Let $\PB$ be the path system comprising all shortest paths in $G^B$ ending in a sink node.
A hitting set for $\PE$ (in $G$) can be used to obtain a hitting set for $\PB$ (in $G^B$), but, since the augmented graph has more information, the sparsity increases.
 
\begin{proposition}\label[proposition]{prop:lshsaug}
Given a  $(\hc,r)$-EPHS for the path system $\PE$, in polynomial time we can construct a $(\hc B,r)$-LSHS for $\PB$.
\end{proposition}
\begin{proof}
Given $C$, an $(\hc,r)$-EPHS for $\PE$, define
\begin{equation}\label{eq:hitset}
C^B\defeq \{\pp{v,b}: v\in C, v \text{ hits }\bar P\in\PB_r, c(\bar P)=b\leq B \}.
\end{equation}
We prove that $C^B$ hits $\PB_r$ and is locally sparse. 
By Proposition~\ref{prop:shorteffic}, we know that shortest paths are efficient, hence $C^B$ hits all the desired paths.
Finally, we prove local sparsity.
Take any node $\pp{s,b}$ and observe that
\begin{equation}\label{eq:local_sparsity}
\Bf_{2r}(\pp{s,b}) = \{\pp{t,x}: \exists P\in\Pst, \ell(P)\leq 2r, c(P)=b-x\} 
\subseteq \{\pp{t,x}: t\in \Bf_{2r}(s), x\leq b\} . 
\end{equation}
We know that $\card{\Bf_{2r}(s)\cap C}\leq \hc$, therefore $\card{\Bf_{2r}(\pp{s,b})\cap C^B}\leq\hc b\leq \hc B$.
A similar argument shows the sparsity for the reverse ball.
\end{proof}

The proof above shows a stronger result:
In \cref{eq:local_sparsity} we see that the sparsity around the node $\pp{u,b}$ is $\hc b$.
This is key for our subsequent query time guarantees.

Surprisingly, in this case we can also relate the HDs of the path systems $\PE$ and $\PB$.
Note that this does not follow from \cref{prop:lshsaug}, since the HD is a stronger notion than existence of locally-sparse hitting sets.
\begin{proposition}\label[proposition]{prop:HDaugmented}
If the HD of the system $\PE$ is $h_c$, then the HD of the system $\PB$ is $Bh_c$.
\end{proposition}
\begin{proof}
Fix $r>0$ and $\pp{v,b}\in V^B$ .
Let $H_{v,r}\subseteq V$ be the set hitting $S_r(v,\PE)$ and define $H\defeq H_{v,r}\times\{0,1,\ldots,B\}$.
We show that $H$ hits $\Sf_r(\pp{v,b},\PB)$.

Take $P\in\Sf_r(\pp{v,b},\PB)$.
Since $\dist(\pp{v,b},P)\leq 2r$, it holds $\dist(v,\bar P)\leq 2r$, therefore $\bar P\in \Sf_r(v,\PE)$.
Finally, $H_{v,r}$ hits $\bar P$, thus $H$ hits $P$.
A similar argument shows that $H$ hits $\Sb_r(\pp{v,b},\PB)$.
\end{proof}


\subsection{Solving CSP via Hub Labels}
\label{ssec:hlcsp}
We present a construction similar to HL for shortest-paths (cf. \cref{ssec:hldef}).
A subtle difference is that we are only interested in paths ending in a sink node.
Each node $\pp{v,b}$ has a forward hub label $\Lf(\pp{v,b})\subseteq V^B$, and \emph{only sink nodes} $u^-$ have a reverse hub $\Lb(u^-)\subseteq V^B$.
The cover property must be satisfied for every $\pp{s,b}$ and $t^-$.
Finally, if we want to reconstruct the path, we can proceed similarly as in ~\cref{ssec:hldef}; we can augment the hub labels with the next-hop node, and compute the entire path recursively.
Putting things together, we can construct hub labels for answering CSP queries, such that their preprocessing time and storage is parameterized by the CHD $h_c$.

\subsubsection{Query Time and Data Requirements}

\begin{theorem}
\label{theo:HLeff}
For a network $(G,\ell,c)$, given a multi-scale EPHS $\crl*{C_i:i=0,1,\ldots,\log D}$, where $C_i$ is an $(h_c,2^{i-1})$-EPHS, we can construct hub labels 
to answer queries for $s,t,b$ in time $O((B+1) h_c\log D)$.
The total space requirement is $\Or(nB \cdot Bh_c\log D)$.
\end{theorem}
\begin{proof}
Create $C_i^B$ as in \cref{eq:hitset}.
Define $L(\pp{v,b})^+ \defeq \bigcup_{i=1}^{\log D} C_i^B\cap \Bf_{2^i}(\pp{v,b})$ and $L(u^-)^-  \defeq \bigcup_{i=1}^{\log D} C_i^B\cap \Bb_{2^i}(u^-)$.
The cover property is proved similarly as in \cref{theo:construct_hl}; we are left to bound the hub size.
For a reverse hub we use that $\Bb_{2^i}(t^-) = \{\pp{s,x}: \exists P\in \Pst, c(P)=x, \ell(P)\leq 2^i\}
\subseteq \Bb_{2^i}(t)\times \{0,1,\ldots,B\}$.
Thus, $\Bb_{2^i}(t^-)\cap C_i^B\leq (B+1)h_c$.
For forward hubs, the size follows from observing that $ \card{C_i^B\cap \Bf_{2^i}(\pp{v,b})}\leq (b+1)h_c$.
\end{proof}


\subsubsection{Preprocessing}
\label{sec:preproc}
Computing hitting sets is difficult in general, but it becomes tractable when the underlying set has small VC-dimension \cite{vc_dim_hitting}.
The critical observation in \cite{highway2013} is that the set system of \emph{unique} shortest paths has a VC-dimension of $2$.
Directed or non-shortest paths break the arguments in \cite{highway2013}, so we need a different formulation of the set system.
We recall the concept of VC-dimension.
A set system $(X,\calX)$ is a ground set $X$ together with a family $\calX\subseteq 2^X$.
A set $Y\in \calX$ is shattered if $\{Z\cap Y:Z\in\calX\}=2^Y$.
If $d$ is the smallest integer such that no $Y\in\calX$ with $\card{Y}=d+1$ can be shattered, then this number $d$ is the VC-dimension of $(X,\calX)$.
The critical observation in \cite{highway2013} is that the set system of \emph{unique} shortest paths has a VC-dimension of $2$, but their argument does not hold in directed graphs or general path systems.

Let $\calQ$ be any path system.
We can obtain a set system with small VC-dimension by considering the ground set as $E$ (instead of the usual choice of $V$), and mapping a path $Q=e_1e_2\ldots e_k$ to $\pi(Q)=\crl{e_1,e_2,\ldots,e_k}$.
Note that, since path systems contain no cycles, each set $\crl{e_1,e_2,\ldots,e_k}$ corresponds uniquely to one path.
\begin{proposition}\label[proposition]{prop:vc_dim}
Given a path system $\calQ$, the corresponding set system $(E,\{\pi(Q):Q\in\calQ\})$ has VC-dimension 2.
\end{proposition}
Note that this argument also can be used for shortest paths in undirected graphs to remove the uniqueness requirement.
Finally, polynomial-time preprocessing now follows from combining \cref{prop:vc_dim} and \cite{vc_dim_hitting}.
The desired result is stated in \cref{prop:poly_lshs}, we defer the proof to Appendix~\ref{sec:proofs}.

\begin{proposition}\label[proposition]{prop:poly_lshs}
If a path system $\calQ$ has HD $h$, then, for any $r>0$, we can obtain in polynomial time a $(h',r)$-LSHS, where $h'=\Or(h\Delta\log(h\Delta))$.	
\end{proposition}

\subsubsection{Using the size of the Pareto Frontier}
\label{sec:frontier}
The linear dependence on $B$ in the bound on HL sizes (cf. \cref{theo:HLeff}) is somewhat weak. 
Essentially, this corresponds to a worst-case setting where the efficient paths between any pair of nodes is different for each budget level. 
In most practical settings, changing the budget does not change the paths too much, and ideally the hub label sizes should reflect this fact. 
This is achieved via a more careful construction of hub labels, resulting in the following bound. 
\begin{theorem}\label{thm:markedhubs}
Let $(G,\ell,c)$ as in \cref{theo:HLeff} and $g:\N\to \N$ be such that, for every $s,t\in V$, $b\in \N$, $\abs{\crl{P\in\Pst^E: c(P)\leq b}} \leq g(b)$.
Then, we can construct hub labels of size $O(g(B)h_c\log D)$, and answer queries with budget $b$ in time $O(g(b)h_c\log D)$.
\end{theorem}

Note that there always exists such a function $g$ and the worst case is $g(b)=B$.
The proof depends on a different technique for constructing HL (refer to \cref{alg:forwardhub,alg:reversehub} in \cref{sec:proofs}). 
The main idea is to sort the efficient paths for each source node $s$ by cost, and then carefully mark nodes when they are added to forward HL; these marked nodes are then used to construct the reverse HL.
For brevity, the complete algorithmic details and proof are deferred to \cref{sec:proofs}.

\subsection{Comparing HD and CHD}
\label{ssec:hdvschd}
The previous sections show that we can construct hub labels for solving CSP whose preprocessing time, storage and query time can all be parameterized in terms of the constrained highway dimension $h_c$. 
This, however, still does not give a sense of how much worse the hub labels for the CSP problem can be in comparison with those for finding shortest paths. 
We now try to understand this question.
Comparing the size of the \emph{optimal} hub labels for SP and CSP is infeasible as even finding the optimal hub labels for SP is NP-hard~\cite{babenko_hl_complexity}. 
However, since we can parametrize the complexity of HL construction for SP in terms of the HD, a natural question is whether graphs with small HD also have a small CHD. 
Note that the answer to this depends on both the graph and the costs.
We now show that the CHD and, moreover, the sparsity of any EPHS, can be \emph{arbitrarily worse} than the HD. 
\begin{figure}
\input{TexImg/big_chd.tex}
\caption{Graph with small HD but large CHD: The graph comprises $2n+1$ nodes, with the edge labels representing the lengths. Note that the shortest paths in the graph are of the form $sv_i$, $u_is$ and $u_isv_j$ (for all combinations $i,j$). Thus, the HD is $1$ as $H_{v,r}=\crl{s}$ is a hitting set for all these paths. On the other hand, if we have costs such that $c(u_iv_i)=0\,\forall\,i$, while all other edges have cost $1$, then we have $n$ parallel efficient paths $u_iv_i$, which must all be hit by any EPHS.}
\label{fig:big_chd}
\end{figure}
\begin{proposition}\label[proposition]{prop:example_bad_chd}
There are networks with HD $1$ where the CHD, and also the sparsity of an EPHS, is $n$.
\end{proposition}
\begin{proof}
Consider the directed graph $G$ defined in Figure~\ref{fig:big_chd}.
It is easy to see that $H_{v,r}=\crl{s}$ is a shortest-path hitting set for every $r>0$ and $v\in V(G)$; hence the HD is $1$.
On the other hand, suppose the costs are such that $c(u_iv_i)=0$ for every $i$, while all other costs are set to $1$.
Note that the $1$-significant efficient paths intersecting the ball $B_s(2)$ are $u_iv_i$, which are all disjoint.
Therefore, the hitting set $H_{s,1}$ must contain at least $n$ elements. 
This concludes the separation between HD and CHD.
Finally, the same argument shows that there is LSHS for $\PS$ with sparsity $1$, whereas the sparsity of \emph{any} EPHS is also lower bounded by $n$.
Thus the sparsities of LSHS and EPHS are also separated. 
\end{proof}

\begin{remark}
One criticism of the graph in \cref{fig:big_chd} is that it has a maximum degree of $n$.
However, the result holds even for bounded degree graphs.
In \cref{app:generalhd}, where we discuss alternative notions of HD, we give a more involved example with bounded degrees that exhibits the same separation between LSHS and EPHS.
\end{remark}

Intuitively, the separation between HD and CHD (or more particularly, the hub labels for computing SPs and CSPs) occurs due to the fact that, for arbitrary graphs and cost functions, the shortest and efficient paths may be completely unrelated. 
For real-world networks, however, this appears unlikely.
In particular, intuition suggests that efficient paths largely comprise of segments which are in fact shortest-paths in their own right. 
This notion can be formalized via the following definition of a \emph{partial witness} 
\begin{definition}[Partial Witness]\label[definition]{def:partial_witness}
Let $\beta\geq 0$.
We say that a path system $\calQ$ is $\beta$-witnessed by the path system $\calQ'$ if, for every $Q\in\calQ$, $\exists Q'\in\calQ'$ such that $Q'\subseteq Q$ and $\ell(Q')\geq 2^{-\beta}\ell(Q)$.
\end{definition}

The partial-witness property and \cref{theo:witness_doubling} provide a nice link between the CHD and HD, essentially showing that scaling CSP queries is of similar complexity to scaling SP queries as long as any efficient path contains large shortest-path segments. 
As an example, consider a \emph{multimodal mapping service} which gives transit routes combining walking, bus and subway, while ensuring at most $k$ transfers. 
For $k=3$, each route has at most $4$ segments, which gives use the partial witness property with $\beta = 2$. 
Now if each individual network has small HD, then the CHD is also small (cf. \cref{prop:union_sets}).

We can now ask if the hub labels for computing SPs and CSPs can be related in settings where the shortest-path system $\PS$ is a partial witness for the efficient path system $\PE$.
At an intuitive level, the partial witness property says that efficient and shortest paths are not completely different, i.e., if $Q$ is efficient, a fraction $2^{-\beta}$ of $Q$ is a shortest path.
As a consequence, a node hitting numerous paths in $\PS$, should also hit many paths in $\PE$.
Note that asking for the witness property to hold for all lengths is too extreme, as this essentially requires that all single-hop paths with 0 costs are shortest paths.
Thus, we want this property only for `long-enough' paths. 

We now show that if, for some $\beta$, the network indeed has the partial witness property for paths longer than some $r_\beta$, then we can relate the HL sizes for the two problems in terms of $\beta$ and the doubling dimension $\alpha$. 
Note that the doubling dimension depends on $G$ and $\ell$; the partial witness property depends on the interplay between $G$, $c$ and $\ell$.
Observe also that, if $\alpha$ is a constant, then the requirement in \cref{theo:witness_doubling} is for paths longer than $r_\beta\sim h\alpha^{\beta-2}$.
\begin{theorem}\label{theo:witness_doubling}
Assume $G$ is $\alpha$-doubling and $\PE_r$ is $\beta$-witnessed by $\PS$ for every $r\geq r_\beta$, where $r_\beta=2^{\log_\alpha(h\alpha^{\beta-2})}$. 
Then, for any $r>0$, given an $(h,r)$-LSHS, we can construct, in polynomial time, an $(h\alpha^{\beta},r)$-EPHS for $(G,\ell,c)$.
\end{theorem}

\begin{proof}
For any $r$, we need to construct a hitting set $C^E$ for $\PE_r$.
Assume first $r\geq r_\beta$.
Let $C$ be the hitting set for $\PS_{2^{-\beta}r}$ which is guaranteed to be sparse with respect to balls of radius $2^{-\beta+1}r$.
Define the desired set by
$
C^E\defeq\{v\in C: v \text{ is in some $r$-efficient path} \}.
$

Since $\PS$ is a $2^{-\beta}$-witness for $\PE_r$, $C^E$ is indeed a hitting set for $\PE_r$.
We are only left to prove the sparsity.
Take some $u\in V$, by doubling dimension we can cover $\Bf_{2r}(u)$ by at most $\alpha^\beta$ balls of radius $2^{-\beta+1}r$.
Each of these balls contains at most $h$ elements of $C$, therefore the sparsity is as claimed.
The argument for reverse balls is identical.

Now we analyse the case $r< r_\beta$.
It is no longer true that efficient paths are witnessed, but now the neigborhoods are small.
We first claim that, for any $v\in V$ and $r>0$, $\abs{B_{r}(v)}\leq \alpha^{\log_2r+1}$.
Indeed, using the doubling property $\log_2r+1$ times, we can cover $B_r(v)$ with balls of radii $1/2$.
Since the minimum edge length is $1$, all of these balls must be singletons and the claim follows.
Now we can take $C=V$ as the EPHS.
Clearly $C$ hits all the paths and the local sparsity is at most the size of the ball.
Using our assumption on $r_\beta$, a simple computation shows that $\card{B_{2r}(v)}\leq \alpha^{\log_2r_\beta+2}\leq h\alpha^\beta$. 
\end{proof}
\begin{remark}
The existence of a $\beta$-witness is not enough to bound the CHD. 
Nevertheless, as we discussed in \cref{ssec:hldef}, the existence of $(h,r)$-LSHS already allows the construction of HL. Moreover, the above argument does indeed give a bound for a weaker definition of the highway dimension~\cite{highway2010}.
\end{remark}

\subsection{Average-Case Performance Guarantees}
\label{sec:avg_hd}
Converting the partial-witness condition to a more interpretable condition is difficult in general, as the structure of $\PS$ and $\PE$ may be complex. 
One way to get such a condition, however, is by considering \emph{average-case} performance metrics.
For this, we relax the definition of HD in two ways: $(i)$ we require LSHS to be locally sparse ``on average'' over all nodes, and
$(ii)$ we only require the existence of LSHS (as opposed to a hitting set for $S_r(v,\calQ)$).

\begin{definition}[Average LSHS]
Given $r>0$ and a system $\calQ$, a set $C\subseteq V$ is an average $(h,r)$-LSHS if it hits $\calQ_r$ and is locally sparse in average, i.e.,
$\frac{1}{n}\sum_{v\in V} \abs{B_{2r}(v)\cap C} \leq h$.
\end{definition}
\begin{definition}[Average HD]
The system $\calQ$ has average HD $h$ if, for every $r>0$, there exists an average $(h,r)$-LSHS.
\end{definition}

From the definition is clear that average HD is a strictly weaker property than HD; in particular, as we discuss before, the existence of LSHS does not guarantee their efficient computation.
Nevertheless, the above definition turns out to suffice to parametrize the average behavior of HL:
\begin{theorem}\label{theo:preproc_avg}
If $\PS$ has average HD $h$, then we can obtain, in polynomial time, HL with average size 
$\frac{1}{n}\sum_{v\in V} \abs{\Lf(v)} \leq h'\log D$ and 
$\frac{1}{n}\sum_{v\in V} \abs{\Lb(v)} \leq h'\log D$,
where $h'=\Or(\Delta h\log (hn\Delta))$.
\end{theorem}
Note that since query time depends linearly on the hub size, the above result implies both storage and performance bounds.\\
\begin{proof}
We only show how to compute average LSHS, since the construction of HL is the same as in \cref{theo:construct_hl} and the bound for the size easily follows. 
The objective is to obtain a set $C_i$ which is an average $(h',2^i)$-LSHS.
This turns out to be a minimum-cost hitting set problem.
Indeed, we want to solve
\[
C_i = \arg\!\min\crl*{\sum_{v\in V}\abs{B_{2^{i+1}}(v)\cap C}: C\subseteq V, C \text{ hits } \PS_{2^i}}.
\]
This follows from a symmetry argument, assigning to each node $u$ the cost $c(u)=\abs{\crl{v\in V: u\in B_{2^{i+1}}(v)}}$.
On the other hand, given a minimum cost hitting set problem with optimum value $\tau$, if the set system has VC-dimension $d$, the algorithm in \cite{vc_dim_hitting} finds a solution, in polynomial time, with cost at most $\Or(d\tau\log(d\tau))$.

By assumption, the minimum of the problem is at most $hn$.
Now we perform a mapping from paths to sets, where the ground set is $E$ and paths are sequences of edges.
This system has VC-dimension 2, and now the minimum is at most $h\Delta n$.
We apply the algorithm in \cite{vc_dim_hitting} and obtain a solution $C_i$ with cost at most $\Or(h\Delta n\log(h\Delta n))$; this gives the promised average $(h',2^i)$-LSHS.
\end{proof}

\subsubsection{Relaxed Witness}
\label{sec:relaxed_witness}
We describe a realistic setting where we can obtain small, in average, HL for the CSP.

First, we assume that individual edges are shortest paths, which is clearly true in road networks, and add an additional constraint wherein we insist our efficient paths have bounded \emph{stretch} compared to the shortest path; note that this is natural in applications as users do not want to be presented solutions which are far away from the optimum, even if it saves them budget.
Formally, we define:

\begin{definition}[Stretch]
	An algorithm for CSP has stretch $\St\geq 1$ if, $\forall s,t\in V$ and $b\leq B$, outputs $\dist(s,t|b)$ whenever $\dist(s,t|b)\leq \St\dist(s,t)$ and outputs ``infeasible'' when $\dist(s,t|b)>\St\dist(s,t)$.
\end{definition}
We henceforth treat $\St$ as an extra constraint given by the application. 
Next, let $\Ec\defeq\crl{e\in E:c_e>0}$ denote the set of `costly' edges.
We define the following notion of an \emph{overpass}:

\begin{definition}[Overpass]
For $r>0$, the edge $e=(u,v)$ is an $r$-overpass if:\\
$(1)$ $e$ belongs to a path $Q\in\PE_{2r}\setminus\PS$\\
$(2)$ both $u$ and $v$ are endpoints of paths in $\PS_{r(2/\St-1)}$ and\\ 
$(3)$ $\min(\dist(e,\Ec),\dist(\Ec,e))\leq 3r/2$
\end{definition} 
Essentially, overpasses are edges connecting long shortest-paths in a costly zone; \cref{fig:overpass} shows an example. In case costs are contiguous (for example, tolls on highways or traffic jams), then the definition corresponds to the intuitive notion of an overpass.
Our main requirement is the following \emph{bounded growth} condition, controlling the number of $r$-overpasses for every scale $r>0$.

\begin{figure}[!b]
	\centering
	\includegraphics[scale=0.7]{TexImg/overpass.pdf}
	\caption{Edge $uv$ here is an overpass, lying on efficient path $Q$ (dashed red), connecting a pair of long shortest paths $P(s,v)$ and $P(u,t)$, and close to costly edges $\tilde P$ (solid green). } 
	\label{fig:overpass}
\end{figure}


\begin{definition}[Bounded Growth]
$(G,c,\ell)$ satisfies the bounded growth condition if, $\forall r>0$, $\abs{\crl{u\in V: \exists v, uv \text{ is an $r$-overpass}}}\leq \phi(2r)$, where $\phi(r)\defeq nh\alpha^{\beta-2-\log_2r}$
\end{definition}

Observe that $\phi$ is a slowly decreasing function of $r$ and, even when $r=D$, we allow for overpasses.
Now, with these conditions, we get our main result of this section:


\begin{theorem}\label{theo:overpasses}
Let $(G,\ell,c)$ be a network with HD $h$.
If the bounded growth is satisfied, we can obtain, in polynomial time, hub labels for CSP queries that guarantee average query-time $\Or((b+1)h'\alpha \log D)$ and total storage $\Or(nB\cdot Bh'\alpha \log D)$, where $h'=\Or(\Delta h\log(hn\Delta))$.
\end{theorem}

We henceforth refer to the average HD of $\PE$ as average CHD.
Since the average HD is enough to obtain good preprocessing algorithms (cf. \cref{theo:preproc_avg}), an average CHD should intuitively suffice.
However, to prove \cref{theo:overpasses}, we need to show that it is possible to obtain small average CHD under even less restrictive assumptions than the partial-witness.
To do this, we first allow for an additional \emph{supplementary witness set} $D$ such that \emph{every efficient path is either witnessed by a shortest-path or hit by $D$}.
This corresponds to the intuition that a few bad efficient paths should not completely ruin the algorithm. 

\begin{definition}[Weak Partial Witness]
Given $\beta\geq 0$, we say a path system  $\calQ$ is weakly $\beta$-witnessed by the path system $\calQ'$ if, for every $r>0$, $\exists D_r\subseteq V$ such that, $\forall Q\in \calQ_r$ either (1) $Q$ is $\beta$-witnessed by $\calQ'$ or (2) $Q$ is hit by $D_r$.
Additionally we require $\abs{D_r}\leq \phi(r)$.
\end{definition} 

Intuitively, the supplementary witness set $D_r$ takes care of all the corner cases where $\calQ$ and $\calQ'$ differ too much.
We now show that our requirement on $\abs{D_r}$ guarantees a bound on the average HD.

\begin{proposition}\label[proposition]{prop:lay_witness}
Assume that $G$ is $\alpha$-doubling and let $\calQ'$ be weakly $\beta$-witnessed by $\calQ$.
If the HD of $\calQ$ is $h$, then the average HD of $\calQ'$ is $h'\leq 2h\alpha^{\beta}$
\end{proposition}
\begin{proof}
Let $r>0$, and $C$ be an $(h,2^{-\beta}r)$-LSHS for $\calQ$.
We show that $C\cup D_r$ is an average $(h,r)$-LSHS for $\calQ'$.
Clearly is a hitting set for $\calQ'_r$ and we can compute
\begin{align*}
h'&\leq \frac{1}{n}\sum_{v\in V} \abs{B_{2r}(v)\cap (C\cup D_r)}\\
&\leq \frac{1}{n}\prn*{\sum_{v\in V} \abs{B_{2r}(v)\cap C}+\sum_{v\in V} \abs{B_{2r}(v)\cap D_r}}\\
&\leq \frac{1}{n}\prn*{\sum_{v\in V} h\alpha^\beta+\sum_{v\in D_r} \abs{B_{2r}(v)}}
\leq h\alpha^\beta + \frac{\abs{D_r}\alpha^{\log_2r+2}}{n}.
\end{align*}
In the third inequality we used that $C$ is sparse with respect to balls of radius $2^{-\beta+1}r$ and that $\sum_{v\in V} \abs{B_{2r}(v)\cap D_r}=\sum_{v\in D_r} \abs{B_{2r}(v)}$ by symmetry of the bi-directional balls.
In the last inequality we used that, by doubling dimension, balls of radius $r$ have at most $\alpha^{\log_2r+1}$ elements.
Since $\abs{D_r}\leq \phi(r)$, the result follows.
\end{proof}

This now allows us to link $\PE$ and $\PS$ as follows:
\begin{proposition}\label[proposition]{prop:1witness}
Under the bounded growth condition, $\PS$ is a weak $1$-witness for $\PE$.
\end{proposition}
\begin{proof}
We first need some additional notation:
For a path $Q$ and two vertices $u,v\in Q$ we denote $Q[u,v]\subseteq Q$ as the sub $(u,v)$-path; for two paths $P,Q$ with a common endpoint, we denote $P|Q$ as their concatenation.

Consider $Q\in\PE$ with endpoints $s,t$ and set $\ell(Q)=2r$.
We will show how to obtain a vertex for the supplementary witness set in case $Q$ is not witnessed and then we bound the size of the set.
Assume that $Q\neq P(s,t)$, otherwise the path is trivially witnessed.
Let $(u,v)\in Q$ be such that $\ell(Q[s,v]),\ell(Q[u,t])\geq r$ (see \cref{fig:overpass}). 
If either $Q[s,v]$ or $Q[u,t]$ is a shortest path or $\ell_{uv}\geq r$, then $Q$ is witnessed. Thus, we henceforth assume that all of the above conditions fail.

We claim that $uv$ is a $r$-overpass (in fact, this is the exact scenario depicted in \cref{fig:overpass}).
Condition 1 is clearly satisfied. 
Condition 2 also holds because both $Q[s,v]$ or $Q[u,t]$ have stretch at most $\frac{2-\St}{\St}$.
To see this, note that both $P(s,v)|Q[v,t]$ and $Q[s,u]|P(u,t)$ are no shorter than $P(s,t)$ and $\St\ell(P(s,t))\geq \ell(Q)$, hence
$\frac{2r}{\St}\leq \ell(P(s,t)) \leq  \ell(P(s,v)) + \ell(Q[v,t])$ and $\frac{2r}{\St} \leq \ell(P(s,t)) \leq  \ell(Q[s,u]) + \ell(P(u,t))$.
Since each path $Q[s,u],Q[v,t]$ has length less than $r$, it follows that both $P(s,v),P(u,t)$ have length strictly greater than $\frac{2-\St}{\St}r$. % and condition 1 follows. 
Finally, to show condition 3, we have $\ell(Q[s,v])+\ell(Q[u,t])=2r+\ell_{uv}$ and since $\ell_{uv}<r$, one of $Q[s,v]$ or $Q[u,t]$ has length at most $3r/2$.
Since neither of these paths is shortest, it must be that both $P(s,v),P(u,t)$ have costly edges and thus one of $u,v$ is closer than $3r/2$ to $E_1$ and the condition is satisfied.

For every path of length $2r$, we can thus either exhibit a witness or show that it contains an overpass and add use the tail of the edge as a supplementary witness.
For a fixed $r>0$, we need to add at most $\phi(r)$ nodes to $D_r$ to cover all the efficient paths. 
The result follows.
\end{proof}

We are almost ready to prove that bounded growth allows to solve the CSP.
The last piece is \cref{prop:avg_chd}, the proof of which follows form similar arguments as those in \cref{theo:HLeff,theo:preproc_avg}.

\begin{proposition}\label[proposition]{prop:avg_chd}
If the average HD of $\PE$ is $h_c$, then we can construct, in polynomial time, hub labels for CSP, which guarantee average query-time $\Or((b+1)h_c'\log D)$ for queries with budget $b$, and total storage requirements $\Or(nB\cdot Bh_c'\log D)$.
\end{proposition}

\begin{proofof}{\cref{theo:overpasses}}
We argue that the average CHD is $h_c\leq 2h\alpha $.
By \cref{prop:1witness}, $\PE$ is weakly $1$-witnessed by $\PS$.
It follows by \cref{prop:lay_witness} that the average CHD is at most $2h\alpha$ as needed.
Applying \cref{prop:avg_chd} yields the result.
\end{proofof}

\section{Scalable CSP Algorithms: Implementations and Experiments}
\label{sec:numeric}
Our theoretical results in the preceding sections suggest that using hub labels for CSP queries should perform well in road networks, as these are known to have low highway dimension, and potentially also satisfy the (average) partial witness property. 
We use our theoretical findings to guide the construction of practical algorithms; the modifications of the theoretical algorithms are simply to replace some of the black box routines.
We now describe how our techniques can be adapted to give practical hub label constructions, and discuss experimental results for two real world networks using these methods. 
\subsection{Practical CSP Algorithms} 
\label{ssec:practical}

We start by defining a more scalable construction of $G^B$.
The augmented graph $G^B$ defined in \cref{ssec:aug} is not a minimal representation as it may contain a lot of redundant information.
For example, the same efficient path $uv$ can be repeated many times in the form $\pp{u,1}\pp{v,0}$, $\pp{u,2}\pp{v,1}$ and so on.
By encoding this information more efficiently, we get considerable improvements both in query time and in data storage.

We construct our \emph{pruned} augmented graph $\Gp^B$ as follows:
As before, nodes are pairs $\pp{v,b}$, but now we add an edge $\pp{v,b}\pp{v',b'}$ only if it is essential for some efficient path, i.e., removing said edge impacts correctness.
If we let $\Pst^E$ be the set of all efficient paths from $s$ to $t$, we take every $P\in \Pst^E$ with cost $b\leq B$ and trace it in the augmented graph such that it terminates at $\pp{t,0}$.

\begin{definition}\label[definition]{def:pruned_aug_graph}
The pruned augmented graph is defined by $\Gp^B=(\Vp^B,\Ep^B)$, where
\begin{align*}
\Vp^B &:=\crl{\pp{v,b}: v\in V, b=0,1,\ldots,B},\\
\Ep^B &:=\crl{\pp{v,b}\pp{u,x} : \exists s,t\in V, P\in\PE_{s,t}, c(P)\leq B, vu\in P, b=c(P[v,t]), x=c(P[u,t]) }.
\end{align*}
In $\Gp^B$ all the lengths are preserved.
\end{definition}
Note that in $\Gp^B$ there are no sink nodes, hence it has at least $n$ nodes and $nB$ arcs fewer compared to $G^B$.
In the worst case, those $nB$ arcs are the only gain by doing this process.
Nevertheless, in our experiments $\Gp^B$ is up to 60\% smaller than $G^B$.
Observe that, by running Dijkstra in $G^B$, $\Gp^B$ can be computed in time $\Or(n^2B\log(nB))$.

\subsubsection{HD of the pruned augmented graph}

A shortest path in $\Gp$ does not necessarily project to an efficient path, even if the path ends in a node of the form $\pp{t,0}$.
In contrast, if $P$ projects to an efficient path, then necessarily $P$ is shortest. 
To bound the HD, the correct system to study is
\[
\tilde\PB:=\crl{P: P\text{ ends in a node }\pp{t,0}, \bar P\in \PE, c(\bar P)\leq B }.
\]
The following result shows how the HD of this system relates to that of $\PE$.
We omit the proof since it is identical as the one in Proposition~\ref{prop:HDaugmented}.
\begin{proposition}
Given CHD $h_c$, the HD of $\tilde\PB$ is $Bh_c$.
\end{proposition}

\subsubsection{Types of queries}

We test our algorithms with two different tasks.
Recall that our preprocessing is done for some fixed maximum budget $B$.
The first task we consider is a \emph{frontier query}, wherein given $s$ and $t$, we return the lengths of all efficient paths with costs $b=0,1,\ldots,B$.
The second we call a \emph{specific query}, we return $\dist(s,t|b)$ for given $s,t,b$ (i.e., a single efficient path).

Note that the pruned augmented graph $\Gp^B$ is designed for frontier queries.
To see this, fix the terminal $\pp{t,0}$.
As we ask for the shortest path from $\pp{s,B},\pp{s,B-1},\ldots,\pp{s,0}$ we are guaranteed to recover the entire frontier.
On the other hand, it may be that the shortest path between $\pp{s,b}$ and $\pp{t,0}$ does not correspond to $\dist(s,t|b)$.
This occurs when $b$ is not a tight budget and the efficient path requires less.

To answer specific queries, we modify $\Gp^B$ by adding extra edges.
For every $v\in V(G)$ and $b=1,2,\ldots,B$, we include the edge $\pp{v,b}\pp{v,b-1}$ with length $0$.
A simple argument shows that with the added edges, the shortest path between $\pp{s,b}$ and $\pp{t,0}$ has length $\dist(s,t|b)$.

\subsubsection{HL construction via Contraction Hierarchies}

We use some techniques described in \cite{hubimplem} combined with an approach tailored for augmented graphs.
The CH algorithm takes as input any ranking (i.e., permutation) of the nodes, and proceeds by removing nodes from the lowest rank first.
Whenever a node is removed, we add new edges, called shortcuts, if needed to preserve the shortest paths.
Once we have a graph with shortcuts, a CH search is a special variant of Dijkstra where only higher rank nodes are explored, i.e., we never take an edge $uv$ if rank$(u)>\text{rank}(v)$. 
The main idea in our construction is to choose an appropriate ranking, and then define the forward hubs of $v$ as the nodes visited during a contraction-based forward search starting at $v$.
The reverse hubs are defined analogously.
These are valid hubs, since the highest rank node in a path is guaranteed to be in both hubs.

The choice of the ranking function is crucial.
For our experiments, we ranked nodes in $G$ by running a greedy approximate SP cover, selecting the highest rank node as the one covering most uncovered paths in $\PS$ and continuing greedily. 
Specifically, start with a cover $C=\varnothing$ and compute the set of all shortest paths $\PS$.
Take a node $v\notin C$ hitting most paths in $\PS$, then remove all those paths from $\PS$, add $v$ to $C$ and iterate.
The rank is defined as $n$ for the first node added to $C$, $n-1$ for the second and so on.
To implement the SP cover we follow the algorithm in \cite{hubimplem}.
A practical hurdle in such an approach is that to compute a shortest path cover, a direct approach requires storing all the shortest paths in memory, which, in most mapping applications, is infeasible.
To circumvent this, we approximate the shortest-path cover by computing only $k<<n$ shortest-path trees and covering these greedily.
We use an off-the-shelf clustering method to obtain $k$ cluster centers, from which we compute the shortest paths. 
As shown in Figure~\ref{fig:clusters}, the clustering approach provides a very good approximation of the hubs with even a small $k$.
We stress that this is just an easy way to get a ranking; more sophisticated heuristics that decide on-line the next node to contract usually work well in practice \cite{goldberg_survey,rice_csp}.
Using another contraction scheme may expedite our algorithms and reduce the hub size.


\begin{figure}
\resizebox{\columnwidth}{!}{%
\begin{minipage}{0.5\columnwidth}
\centering
\input{TexImg/plot_clusters_sf.tex}
\end{minipage}%
\begin{minipage}{0.5\columnwidth}
\centering
\input{TexImg/plot_clusters_lu.tex}
\end{minipage}
}
\caption{Performance of clustering for San Francisco (left) and Luxembourg (right). 
In the $y$-axis the quantities are normalized by the best greedy cover, i.e., using $k=n$.
Note that, while the number of short-cuts is an indicator of performance, it is not perfectly correlated with the hub size.}
\label{fig:clusters}
\end{figure}


Depending on the size of our instance, and the specific queries, we work with either the augmented graph $G^B$ or the pruned augmented graph $\Gp^B$.
Even though $\Gp^B$ takes time to compute, it can speed up the overall process and yield considerably better hubs. 
Given a ranking for nodes in $G$, we contract $\Gp^B$ as follows.
Say that $V$ is ordered according to the ranking, so node~$1$ is the least important and $n$ the most important.
In $\Gp^B$, we first contract the nodes $\pp{1,b}$ for $b=B,\ldots,0$, then the nodes $\pp{2,b}$ and so on till the nodes $\pp{n,b}$ are the last to contract. 
Finally, when contracting a node $v$, if $u$ is a predecessor and $w$ a successor of $v$, we add the short-cut $uw$ only if, by removing $v$, the distance from $u$ to $w$ is altered, and the new shortest path from $u$ to $w$ is efficient.
We can go even further; the short-cut $uw$ is unnecessary if the shortest path is not efficient, even if the distance changes.

To obtain better hubs we prune the results obtained by CH searches.
If $w$ is in the forward search of $v$ with distance $d$, it might be that $\dist(v,w)<d$, this occurs because the search goes only to higher rank nodes and the discovered path is missing some node.
When $\dist(v,w)<d$, we can safely remove $w$ from the hub of $v$, since the highest ranked node in a shortest path will have the correct distance.
For frontier queries, we can also prune away a node $w$ if the $(v,w)$-path has a surplus of budget.
The entire process can be summarized in the following steps.




\begin{enumerate}[nosep]
\item Compute the shortest paths in $G$ and use a greedy approach to obtain a cover $C$
\item Compute the pruned augmented graph $\Gp^B$
\item Contract $\Gp^B$ using the rank induced by $C$
\item Create hubs $\Lf(v),\Lb(v)$ using CH
\item Prune the hubs by running HL queries between $v$ and nodes in $\Lf(v)$. 
Run a similar process for $\Lb(v)$.
\end{enumerate}
Recall that, for some instances, we skip step 2 and contract $G^B$ instead.
Note that in the last step we bootstrap HL to improve it.
This works because the fact that some nodes have incorrect distance labels does not impact the correctness of a HL query; a node minimizing the distance is returned and such node must have a correct label.

\subsection{Experiments} \label{sec:exp}
All our experiments were performed on a 64-bit desktop computer with a 3.40GHz Intel Core i7-6700 processor and 16GB RAM running Ubuntu 16.04.
Our code is written in Python 2.7.
We use the library Networkx for the graph representation and Dijkstra's algorithm.
Although all the steps can be parallelized, we did not implement this. Our complete code (implementation and artificial dataset) is publicly available at \url{github.com/albvera/HHL_CSP}.

We evaluated the performance of our algorithms with real-world test networks: downtown San Francisco with 2139 nodes and 5697 edges for which real-world travel-time data was available as a Gaussian mixture model \cite{sf_data}, and Luxembourg City with 4026 nodes and 9282 edges for which travel-time distributions were synthesized from speed limits \cite{niknami2016tractable}, as real-world data was unavailable.
 
In our experiments, we use the mean travel times for our length function and the following cost structure; the top 10\% of edges with the highest variance are assigned cost 1 and the rest cost 0.
This is a measure of risk, since edges with high variance are prone to cause a delay in the travel time.

\begin{table*}[h]

\begin{adjustbox}{max width=\textwidth,keepaspectratio}
\begin{tabular}{| l | p{1cm} | p{1cm} | p{1cm} | p{1.2cm} | p{1.2cm} | }
\hline
	B & Prepro [m] & Avg F Size & Avg B Size & Query Dij [ms] & Query HL [ms] \\ \hline \hline
	0 & 1 & 23 & 22 & 10.71 & 0.005 \\ \hline
5-f  & 5  & 16 & 28 & 80.10  & 0.02 \\
5-s  & 5  & 57 & 28 & 31.75  & 0.01 \\ \hline
10-f & 9  & 9 & 28 & 168.11 & 0.03 \\
10-s & 10 & 68 & 28 & 56.19  & 0.01 \\ \hline
15-f & 12 & 6 & 28 & 237.64 & 0.03 \\
15-s & 16 & 73 & 28 & 77.59  & 0.01 \\ \hline
20-f & 17 & 5 & 28 & 342.47 & 0.03 \\
20-s & 20 & 77 & 28 & 100.26 & 0.01 \\ \hline
25-f & 22 & 4 & 28 & 460.95 & 0.03 \\
25-s & 25 & 80 & 28 & 126.52 & 0.01 \\ \hline
30-f & 26 & 3 & 28 & 569.13 & 0.03 \\
30-s & 31 & 84 & 28 & 152.75 & 0.01 \\ \hline
\end{tabular}%
\quad
\begin{tabular}{ | l | p{1cm} | p{1cm} | p{1cm} | p{1.2cm} | p{1.2cm} |}
\hline
	B & Prepro  [m] & Avg F Size & Avg B Size & Query Dij [ms] & Query HL [ms] \\ \hline \hline
	0 & 6 & 18 & 18 & 16.35 & 0.004 \\ \hline
5-f  & 1  & 18.0 & 18.0 & 175.04  & 0.02 \\
5-s  & 21 & 42.9 & 18.7 & 72.03   & 0.01 \\ \hline
10-f & 2  & 18.0 & 18.0 & 361.15  & 0.04 \\
10-s & 35 & 49.3 & 18.7 & 102.52  & 0.01 \\ \hline
15-f & 3  & 18.0 & 18.0 & 577.89  & 0.06 \\
15-s & 46 & 53.3 & 18.7 & 140.80  & 0.01 \\ \hline
20-f & 4  & 18.0 & 18.0 & 821.94  & 0.07 \\
20-s & 60 & 56.5 & 18.7 & 183.11  & 0.01 \\ \hline
25-f & 5  & 18.0 & 18.0 & 974.84  & 0.09 \\
25-s & 77 & 59.5 & 18.7 & 227.17  & 0.01 \\ \hline
30-f & 7  & 18   & 18   & 1247.72 & 0.10 \\
30-s & 93 & 62.3 & 18.7 & 272.41  & 0.01 \\ \hline
\end{tabular}%
\end{adjustbox}

\caption{Experimental results for San Francisco (left) and Luxembourg City (right). Query times are measured with 1000 random $s,t$ pairs for each network and multiple maximum budget levels $B$. Results on rows $B-f$ correspond to computing the solution frontier for all budgets $b\leq B$ while rows $B-s$ correspond to computing the solution for budget level $b$.}
\label{tab:performance_results}
\end{table*}

\subsubsection{Query-time performance}


Table~\ref{tab:performance_results} presents the CSP computation times for different maximum budgets $B$. 
For frontier queries, labeled as `f', the query times are measured as the average of 1000 random $s,t$.
For specific queries, labeled as `s', the times are measured as 1000 random triplets $s,t,b$.
The column for $B=0$ represents the original graph (without augmentation). 
As can be seen in the experimental results, our method finds the constrained shortest path solution on average four orders of magnitude faster than running Dijkstra's algorithm on the augmented graph. 
Preprocessing for frontier queries results in a more compact set of hub labels, since a node $\pp{s,b}$ needs to store information for paths with budget exactly equal to $b$ (in case the path is efficient, otherwise it is not stored).
On the other hand, for specific queries, $\pp{s,b}$ needs to store information for all budgets up to $b$.
The preprocessing time does not include the cover computation, since this is a flat cost of at most the time to pre-process the instance $B=0$.

Note that preprocessing frontier queries in Luxembourg is faster, despite the network being bigger, this can be explained by the structural properties.
For example, in Luxembourg there are more highways and fast roads.

Observe that the \emph{average hub size decreases} in San Francisco for frontier queries, this is because in this instance we use $\Gp$, which prunes away most of the nodes, thus many nodes $\pp{v,b}$ are isolated and have empty hubs.
The longer preprocessing time for frontier queries can be explained as follows.
There are many cases when two nodes are not reachable, to detect this requires Dijkstra to explore the entire graph.
In contrast, for specific queries we add extra edges $\pp{s,b}\pp{s,b-1}$, hence a reachability test ends, in average, earlier.
In the contraction step, we want to remove a node without altering the shortest path, a process that requires many reachability tests.

\begin{figure}[h]
\begin{minipage}[t]{.28\textwidth}
\centering
\includegraphics[clip, trim=0.2cm 0.3cm 0.4cm 0.2cm,scale=0.225]{TexImg/SF_query_dij_B25.pdf}\\
(a)
\end{minipage}%
\begin{minipage}[t]{.24\textwidth}
\centering
\includegraphics[clip, trim=1.65cm 0.3cm 0.2cm 0.2cm,scale=0.225]{TexImg/SF_query_hl_B25.pdf}\\
(b)
\end{minipage}%
\begin{minipage}[t]{.24\textwidth}
\centering
\includegraphics[clip, trim = 1.3cm 0.3cm 0cm 0cm,scale=0.225]{TexImg/SF_bwd_hub_size.pdf}\\
(c)
\end{minipage}%
\begin{minipage}[t]{.24\textwidth}
\centering
\includegraphics[clip, trim = 1.3cm 0.3cm 0cm 0cm,scale=0.225]{TexImg/significance.pdf}\\
(d)
\end{minipage}
\caption{ Histogram frontier queries for Dijkstra (a) and HL (b). Times are 1000 random pairs in San Francisco augmented with $B=25$.
Size of reverse hubs (c) and significance (d) for frontier queries in San Francisco augmented with $B=25$.
}
\label{fig:SF_query}
\label{fig:SF_bwd_size}
\end{figure}
	

\subsubsection{Hub sizes and node significance}

We focus the analysis on two meaningful quantities.
The first is hub size, which is well captured by  $\card{\Lb(\pp{t,0})}$ for $t\in V$.
Indeed, for frontier queries the reverse hub is bounding the space requirements; for specific queries the same is true up to a constant factor.
For the second quantity, we define the significance of $s\in V$ as the number of hubs containing $s$, i.e., $\sum_t\sum_b\In{\pp{s,b}\in \Lb(\pp{t,0})}$.
Intuitively, a node is highly significant if belongs to many efficient paths.
Figure~\ref{fig:SF_bwd_size} shows a histogram of these metrics. 



\begin{figure}[h]
\begin{minipage}[t]{.53\textwidth}
\centering
\includegraphics[clip, trim=4.6cm 4.8cm 4.7cm 3cm,height=4.7cm]{TexImg/SF_hub_sizes.pdf}%
\includegraphics[clip, trim=15.8cm 0cm 0.4cm 1cm,height=4.4cm]{TexImg/SF_hub_sizes.pdf}\\
(a) Heatmap of hub-sizes for SF
\end{minipage}%
\begin{minipage}[t]{.47\textwidth}
\centering
\includegraphics[clip, trim=3.7cm 2.9cm 4.2cm 3cm,height=4.7cm]{TexImg/sig_colapse.pdf}\\
(b) Heatmap of significance for SF
\end{minipage}
\caption{Heat maps for frontier queries in San Francisco augmented with $B=25$.
On the left we see the hub size. 
Note that the size is not homogeneous, but rather we can observe clusters and neighborhoods tend to be similar.
On the right the significance.
The most significant nodes have been drawn bigger.
The top 3 most significant correspond to Geary Blvd \& Gough St, Franklin St \& O'Farrel St and Market St \& Polk St.
}
\label{fig:heat_maps}
\end{figure}


\begin{figure}
\centering
\includegraphics[scale=0.3]{TexImg/map_LU_sig.png}
\caption{Heat map of significance for frontier queries in Luxembourg City augmented with $B=25$.
Notice how the highly significant nodes are in main road crossings.}
\label{fig:map_LU} 
\end{figure}

\cref{fig:heat_maps} presents the spatial relationships between the hub size and significance in the San Francisco network. 
\cref{fig:map_LU} shows the spatial distribution of significance in the Luxembourg network.
We observe that highly significant nodes tend to have small hub size.
The intuition is simple, if most hubs contain $s$, then is easier for $s$ to satisfy the cover property with a small hub.
Note also that the hub size resembles an harmonic function; nodes are mostly similar to their neighbors.


\section{Conclusions}
\label{sec:conclu}
We introduced a new network primitive, the Constrained Highway Dimension, and used it to parametrize the storage and running time of data structures that support fast CSP queries.
Our aim was to study when efficient SP computation yields a similar performance for the harder CSP problem.
We derived conditions under which this holds and we can compare the HD and CHD.
For the worst-case setting, the conditions are given by the partial-witness and for average-case by the milder bounded growth. 
Both conditions have intuitive interpretations in terms of the physical structure of the network. 

On the practical side, we validated our findings by developing algorithms that performed four orders of magnitude better on real-world networks, compared to standard techniques.
Our work is a first step in bridging the gap between SP and CSP algorithms and we believe our findings are promising for real-world applications.



\begin{APPENDICES}
\section{Different notions of HD}
\label[appendix]{app:generalhd}
The original definition of HD was given by \citep{highway2010}. This is not the one we extend, but rather we work from the definition of~\citep{highway2013}.
The latter is the same as our notion except that $(i)$ we consider a less restrictive definition of path neighborhoods, that is appropriate for our needs, and $(ii)$ we generalize the notion to directed graphs and general path systems.
For $\PS$, a consequence of considering less-restrictive path-neighborhoods is that the highway dimension returned by our definition is smaller than that of \citep{highway2013}.
In particular, unlike \citep{highway2013}, the HD of $G$ as per our definition is not an upper bound to the maximum degree $\Delta$ or the doubling dimension $\alpha$.

With respect to our average HD in \cref{sec:avg_hd}, we note the following.
\begin{remark}
The algorithm in \cref{theo:preproc_avg} makes one call to the VC-dimension solver for each $C_i$.
On the other hand, the algorithm in \citep{highway2013} calls up to $n$ times the solver for each $C_i$.
Finally, there is an extra $\log n$ factor in the approximation guarantee, but now the value of $h$ can be much smaller.
\end{remark}

We now discuss how our results extend to the definition in \citep{highway2013}, which we refer to as strong-HD.
The strong-HD defines a path $P$ to be $r$-significant if, by adding at most one hop at each end, we get a shortest path $P'$ longer than $r$.
The path $P'$ is called an $r$-witness for $P$.
Intuitively, a path is significant if it represents a long path.
Observe that, if $P\in\PS$ is such that $\ell(P)>r$, then $P$ is $r$-significant by definition.
We remark also that a path can have many $r$-witnesses.

Finally, the path neighborhood must also be strengthened.
The path $P\in\PS$ belongs to $\Sf_r(v)$ if, $P$ has some $r$-witness $P'$ such that $\dist(v,P')\leq 2r$.
The reverse neighborhood $\Sb_r(v)$ is defined analogously.
With this modified versions of $r$-significant and neighborhood, the notions of LSHS and HD are the same as our previous definitions.

Under the strong-HD, we have  $\Delta\leq h$ and $\alpha\leq h+1$.
Additionally, this definition allows proving results for CH.
Finally, we show that even for the strong-HD, CHD and HD can still be off by a factor of $n$.

\begin{proposition}\label[proposition]{prop:treelike}
For any $h$, we can construct a family of networks such that the sparsity of LSHS is $h$ and that of EPHS is arbitrarily worse than $h$.
\end{proposition}
\begin{proof}
First, we construct an example where the sparsity grows from $h$ to $h^2$.
Consider an $h$-ary tree rooted at $u$ with three levels, i.e., with $1+h+h^2$ nodes.
Now add a node $v$ with $h$ children as in Figure~\ref{fig:treelike}. 
The grandchildren of $v$ are the same as the grandchildren of $u$.

All the edges are bidirectional and have unit cost.
The lengths are as follows: $ux_i$ and $vy_i$ (dashed in Figure~\ref{fig:treelike}) are zero; $uv$ and from $y_i$ to the leafs is one; from $x_i$ to the leafs is three.
It is easy to see that the sparsity of a LSHS is $h+1$.

On the other hand, every leaf $w$ is a $2$-efficient path.
Indeed, it can be extended to $x_iw$ that is the shortest path from $x_i$ to $w$ with constraint 1.
All the leafs are in the ball $B_4(u)$, so the sparsity is at least $h^2$.

\begin{figure}
\centering
\includegraphics[scale=0.5]{TexImg/Treelike.pdf}
\caption{Example where the EPHS is much larger than the LSHS.}\label{fig:treelike}
\end{figure}

The general case works in the same fashion.
We make the sparsity grow to $h^k$ by creating two complete, $k$-level, $h$-ary trees $T$ and $T'$.
Connect the root of $T$ to the root of $T'$ and the leafs of both trees are shared.
Observe that the number of nodes is 
\begin{align*}
n &=[\text{$k$-level $h$-ary tree}] + [\text{$(k-1)$-level $h$-ary tree}]\\
&= ({h^{k+1}-1})/({h-1}) + ({h^k-1})/({h-1}),
\end{align*}
therefore the sparsity is $\Theta(n)$, the worst possible.
\end{proof}

\section{Contraction Hierarchies}
\label[appendix]{sec:ch}
We present here how to extend the concept of HD in order to prove the efficiency of CH in directed graphs.
Given a rank in the nodes, the shortcut process works as in the non-directed case:
\begin{enumerate}
	\item Let $G'$ be a temporary copy of $G$.
	\item Remove nodes of $G'$ and its edges in increasing rank.
	\item When removing $v$, if some unique shortest path in $G$ uses $uvw$, add $(u,w)$ to $G'$ with length $\ell(u,v)+\ell(v,w)$.
\end{enumerate}

Call $E^+$ the set of edges created in the shortcut process.
A source-destination query runs bidirectional Dijkstra, but each search only considers paths of increasing ranks.

As in the non-directed case, let $Q_i=C_i\setminus \cup_{j>i}C_j$ be the partition of $V$.
All the ranks in $Q_i$ are smaller than those in $Q_{i+1}$, within each $Q_i$ the rank is arbitrary.

\begin{lemma}\label{lemma:intshort}
	Let $P$ be a shortest path in the original graph.
	If $P$ has at least three vertices and $\ell(P)>2^{\gamma}$, then some internal vertex of $P$ belongs to a level $Q_x$, $x>\gamma$.
\end{lemma}
\begin{proof}
	The path $P'$ obtained by removing the endpoints of $P$ is $\ell(P)$-significant.
	By definition of the $C_i$'s, $C_{\gamma+1}$ hits $P'$ at some node $u$.
	By construction of the partition, $u\in Q_x$ with some $x>\gamma$. 
\end{proof}

Now we show that each node adds at most $h$ to its out-degree for each $Q_i$, so the process adds at most $h\log D$ to the out-degree of each node.
\begin{lemma}
	Assume the network admits the $C_i$'s.
	For any $v$ and fixed $j$, the number of shortcuts $(v,w)$ with $w\in Q_j$ is at most $h$.
\end{lemma}
\begin{proof}
	Let $i$ be the level such that $v\in Q_i$ and define $\gamma:=\min(i,j)$.
	We claim that $w\in \Bf_{2^\gamma}(v)$.
	Assume the claim, then the number of shortcuts is at most $\card{Q_j\cap \Bf_{2^\gamma}(v)}$, but using local sparsity and set inclusion:
	\[
	\card{Q_j\cap \Bf_{2^\gamma}(v)}\leq \card{C_j\cap\Bf_{2\cdot 2^{j-1}}(v)} \leq h.
	\]
	
	All that remains is to prove the claim.
	The shortcut $(v,w)$ was created when the process removed the last internal vertex of the shortest path $P(v,w)$ in $G$.
	Necessarily all the internal vertices are in levels at most $\gamma$, because they were removed before $v$ and $w$, hence they have lower rank.
	Finally, apply Lemma~\ref{lemma:intshort} to conclude that $\ell(P(v,w))\leq 2^\gamma$.
\end{proof}

We need to bound the in-degree, because it could be that some node $v$ is receiving many edges.
The proof is basically the same.
\begin{lemma}
	Assume the network admits the $C_i$'s.
	For any $v$ and fixed $j$, the number of shortcuts $(w,v)$ with $w\in Q_j$ is at most $h$.
\end{lemma}
\begin{proof}
	Same as in the previous lemma, but now $w\in \Bb_{2^\gamma}(v)$.
\end{proof}

We can conclude now that the number of shortcuts, i.e.\ $\card{E^+}$, is at most $2nh\log D$.

As we mentioned before, the query performs Dijkstra from the source and target, but always constructing paths of increasing rank.
When scanning a vertex $v$, the forward search has a label $\dist(s,v)'$. 
The labels always satisfy $\dist(s,v)'\geq\dist(s,v)$, but, since the algorithm only goes to higher ranks, equality is not guaranteed.

We add a pruning rule analogous to the non-directed case: when the forward search scans a node $v$, if $(v,w)\in E\cup E^+$ and $w\in Q_i$, then $w$ is added to the priority queue only if $\text{rank}(w)>\text{rank}(v)$ and $\dist(s,v)'+\ell(v,w)\leq 2^i$.
For the reverse search, the condition is the analogous $\dist(v,t)'+\ell(w,v)\leq 2^i$ when $(w,v)\in  E\cup E^+$.

\begin{proposition}
	The query with additional pruning returns the correct distance.
	Additionally, each Dijkstra scans at most $h$ nodes in each level.
\end{proposition}
\begin{proof}
	Let us analyse the forward search.
	Say the node $v$ is being scanned, $w\in Q_i$ is a candidate and $\dist(s,v)'+\ell(v,w)>2^i$.
	If the current path $P'$ to $w$ is optimal, then $P(s,w)$ is $2^i$-significant and it is hit by $C_{i+1}$. 
	As a consequence, $P(s,w)$ contains an internal vertex with higher rank than $w$.
	This vertex cannot be in $P'$ nor a shortcut containing it, thus contradicting the optimality of $P'$.
	We conclude that $P'$ is not optimal and $w$ can be ignored.
	
	Bounding the number of scanned nodes is easy; every $w\in Q_i$ added to the queue satisfies $w\in \Bf_{2^i}(s)$, so applying local sparsity we finish the proof.
\end{proof}

As a result, the forward search adds at most $h\log D$ nodes to the queue;
each of node amounts to $O(\text{outdeg} (G^+))$ operations, i.e., $O(\text{outdeg}(G) + h\log D)$ operations.


\section{CHD vs. HD: Extensions}
\label[appendix]{app:extn}
% !TEX root = main_vldb.tex

So far we have only used the structure of shortest paths in $G$, which, naturally, does not capture all the information in the network.
It is natural to think that there is structure if we look at, for example, only zero cost edges.

Let $G_0$ be obtained from $G$ by removing all the edges with cost.
The networks $G$ and $G_0$ define two hierarchies of roads; shortest paths in $G_0$ are free, but not as fast as the ones in $G$.
Our main hypothesis now is that an efficient path $P$ does not alternate between these two hierarchies.
For example, a path that enters and exits multiple times a highway is not desirable because of turning costs.

\begin{proposition}\label[proposition]{prop:union_sets}
Let $\calQ,\calQ'$ be two path systems with HD $h$ and $h'$ respectively.
The HD of the system $\calQ\cup\calQ'$ is at most $h+h'$.
\end{proposition}
\begin{proof}
Given $v\in V$, the union of $H_{v,r}$ and $H_{v,r}'$ hits all the paths in $S_r(v,\calQ)\cup S_r(v,\calQ')$.
\end{proof}

We now relax the assumption that a system witnesses another.
It could be that the efficient paths are sometimes witnessed by free paths and sometimes by shortest paths.

\begin{theorem}
Assume that $G$ has doubling dimension $\alpha$ and $\calQ,\calQ'$ are systems with HD $h,h'$ respectively. 
Moreover, suppose $\PE$ does not alternate between $\calQ$ and $\calQ'$, that is, for some $\beta,\beta'>0$, each path $P\in\PE$ is either $\beta$-witnessed by some $Q\in\calQ$ or $\beta'$-witnessed by $Q'\in\calQ'$. Then $G$ admits  $(\alpha^{\beta}h+\alpha^{\beta'}h',r)$-EPHS.
\end{theorem}


\subsection{Correlated Costs}
We have studied so far the case where $c(P)$ is just the sum of individual edge costs.
In practice it could be that the cost depends on combinations of arcs.
Think of a turn in a road network; we can turn right quickly, but turning left means waiting for a green arrow in most cases.
Another example is minimizing expectation subject to bounded variance.
If there is no independence, the variance of a path is not the sum of individual variances.

We explain now how to deal with more general cases using the same framework.
Assume the cost function $c_2:E\times E\to \N\cup \{0\}$ depends on pairs of edges, so if a path is $P=e_0e_1\ldots e_k$, then the cost would be $c_2(P)=\sum_{i=1}^{k}c_2(e_{i-1},e_i)$.
The nodes in the augmented graph will be triplets $\pp{u,v,b}$, where $v$ is the current state, $u$ is the previous state and $b$ is the available budget.
The arcs are given by
\[
(\pp{u,v,b},\pp{v,w,b'}), \quad uv,vw\in E, b'=b-c_2(u,v,2).
\]

Define analogously the concept of efficient paths.
It is easy to see that, as in the previous case, shortest paths in the augmented graph are efficient paths.
The system $\tilde\PE$ of such paths may also allow for a $\beta$-witness.
With the previous properties we can construct the hub labels in the same fashion to prove the following result.

\begin{theorem}
Assume the system $\tilde\PE$ has HD $\tilde h$.
Then, there exists HL such that queries $s,t,b$ can be answered in time $\Or(b\Delta\tilde h\log D)$ and the space requirement is $\Or(Bn\cdot\Delta B\tilde h\log D)$.
In particular, if $\PS$ is a $\beta$-witness for $\tilde \PE$, then $\tilde h\leq h\alpha^{\beta}$. 
\end{theorem}


\section{Additional Proofs}
\label[appendix]{sec:proofs}
\begin{proofof}{\cref{thm:markedhubs}}
	To get this stronger bound, we need to modify the HL construction. The algorithm for forward hub construction is given in Algorithm~\ref{alg:forwardhub}, and for reverse hubs in Algorithm~\ref{alg:reversehub}. Note that the two must be run sequentially, as the latter uses the nodes marked in the former. We make the forward hubs $\Lf(\pp{v,b})$ slightly bigger by storing, for each node the distance from $\pp{v,b}$ and also the \emph{budget surplus}.
	Let $C_i$ be the $(h_c,2^{i-1})$-EPHS and $\PE_{s,t}$ the efficient paths from $s$ to $t$.
	
	Observe that, whenever a node $v\in C_i$ is added, $v\in \Bf_{2^i}(s)$ guarantees that at most $h_c$ such points are needed for the whole process.
	Additionally, every such $v$ is added at most $g(b)$ times in the hub of $\pp{s,b}$.
	The data requirement guarantee follows.
	
	The bound for data requirements is $g(B) h_c\log D$, the argument is analogous to the forward case.
	Finally, we need to prove the cover property.
	Take any query $SP(\pp{s,b},t^-)$ and let $P$ be the solution.
	In $Lf(\pp{s,b})$ there is a node $v_P$ added by Algorithm~\ref{alg:forwardhub}.
	By construction, the same node $v_P$ was added to $\Lb(\pp{d,0})$. The result follows.
\end{proofof}

\begin{algorithm}
	\small
	\caption{Construction of forward hub}
	\label{alg:forwardhub}
	\begin{algorithmic}[1]
		\Require Node $s\in V$, efficient paths $\PE_{s,t}\,\forall\,t$, EPHS $\crl*{C_i}$.
		\Ensure Forward hubs $Lf(\pp{s,b})$ for $b=0,\ldots,B$ and a marked node $v_P$ for every path.
		\State Order each $\PE_{s,t}$ by increasing cost and remove paths consuming more than $B$.
		\For{$t\in V\setminus s$}
		\For{$P\in\Pst^E$}
		\State $b\gets c(P)$, $b'\gets c(P')$, where $P'$ is the next path in the list ($b'=B$ if no such path).
		\State Find the largest $i$ such that $P$ is $2^{i-1}$-efficient.
		\State Find $v\in C_i$ hitting $P$ and mark $v$ as $v_P$.
		\State Add $\pp{v,c(P[v,t])}$ to $L(\pp{s,b})^+$  with distance $\ell(P[s,v])$ and surplus zero.
		\For{$x$ between $b$ and $b'$}
		\State Add $\pp{v,c(P[v,t])}$ to $L(\pp{s,x})^+$  with distance $\ell(P[s,v])$ and surplus $x-b$.
		\EndFor
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\small
	\caption{Construction of reverse hub}
	\label{alg:reversehub}
	\begin{algorithmic}[1]
		\Require Node $t\in V$, efficient paths $\Pst^E\,\forall\,s$, marked nodes and EPHS $C_i$.
		\Ensure Backward hub $\Lb(\pp{t,0})$.
		\State Order each $\Pst^E$ by increasing cost and remove paths consuming more than $B$.
		\State $\Lb(\pp{t,0})\gets \varnothing$
		\For{$s\in V\setminus t$}
		\For{$P\in\Pst^E$}
		\State Find the largest $i$ such that $P$ is $2^{i-1}$-efficient.
		\State Take $v$ as the marked node $v_P$.
		\State Add $\pp{v,c(P[v,t])}$ to $\Lb(\pp{t,0})$ with distance $\ell(P[v,t])$.
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}


\begin{proofof}{\cref{prop:poly_lshs}}
We extend some arguments from \cite[Theorem 8.2]{highway2013}.
Denote $S_r(v):=\Sf_r(v,\calQ)\cup\Sb_r(v,\calQ)$. 
Observe that, for fixed $v\in V$, the set system $(E,\{\pi(Q):Q \in S_r(v)\})$ admits a hitting set of size $h\Delta$.
Indeed, we know that exists $H_{v,r}\subseteq V$, $\card{H_{v,r}}\leq h$, hitting every path in $\Sf_r(v,\calQ)$ and in $\Sb_r(v,\calQ)$.
The desired hitting set consists of all the edges adjacent to a node in $H_{v,r}$.

If the minimum size of a set system is $s$ and the VC-dimension is $d$, then the algorithm in \cite{vc_dim_hitting} obtains, in polynomial time, a hitting set of size at most $\Or(sd\log(sd))$.
In particular, we can use the algorithm to obtain a set $\tilde F_{v,r}\subseteq E$, of size at most $h'=\Or(h\Delta\log(h\Delta))$, hitting the set system $(E,\{\pi(Q):Q \in S_r(v)\})$ .

Consider the set $F_{v,r}\subseteq V$ that contains all the endpoints of edges in $\tilde F_{v,r}$.
It follows that $F_{v,r}\subseteq V$ can be obtained in polynomial time and is a hitting set for $S_r(v)$ of size $\card{F_{v,r}}\leq 2h'$.
Assume for now that we know the value of $h$.
Note that the value $h'$ can be computed from $h$ and the guarantee given by the oracle, i.e., the constant inside the big-O.
We construct the $(2h',r)$-LSHS iteratively.
At each iteration $i$ we maintain the following invariant: $C_i$ hits every path in $\calQ_r$.
In an iteration we check if $C_i$ is locally sparse, if not, we strictly reduce the cardinality of $C_i$ while maintaining the invariant.
Start with $C_0=V$. 
Let $B_{2r}(v):=\Bf_{2r}(v)\cup \Bb_{2r}(v)$.
Assume $v\in V$ is such that $\card{B_{2r}(v)\cap C_i}>2h'$ and let $C_{i+1}:=(C_i\setminus B_{2r}(v))\cup F_{v,r} $.
The cardinality strictly decreases and we only need to check the invariant.
Consider the paths hit by nodes removed in $C_i$, this set is
\[
\crl{Q\in\calQ_r: Q\cap C_i\cap B_{2r}(v)\neq \varnothing}
\subseteq \crl{Q\in\calQ_r: Q\cap B_{2r}(v)\neq \varnothing} 
\subseteq S_r(v).
\]
Since $F_{v,r}$ hits $S_r(v)$, the proof is completed.

If we do not know the value of $h$, we can do a doubling search for $h'$. 
Indeed, if the guess of $h'$ is low, then at some point it could be that $\card{F_{v,r}}>2h'$, then we double $h'$ and restart the process.
\end{proofof}



\end{APPENDICES}


%%
%\theendnotes

% Acknowledgments here
\ACKNOWLEDGMENT{The authors would like to thank Moritz Kobitzsch for sharing sanitized versions of the San Francisco and Luxembourg road networks.

The authors gratefully acknowledge funding from the US Army Research Laboratory under grant W911NF-17-1-0094, and the National Science Foundation under grant DMS-1839346. 
}


% References here (outcomment the appropriate case)

%\bibliographystyle{informs2014}

% outcomment this and next line in Case 1
%\bibliography{biblio}

\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }

\bibitem[{Abraham et~al.(2013)Abraham, Delling, Fiat, Goldberg,
  \protect\BIBand{} Werneck}]{highway2013}
Abraham I, Delling D, Fiat A, Goldberg AV, Werneck RF (2013) Highway dimension
  and provably efficient shortest path algorithms. \emph{Microsoft Research,
  USA, Tech. Rep} 9.

\bibitem[{Abraham et~al.(2011)Abraham, Delling, Goldberg, \protect\BIBand{}
  Werneck}]{hubimplem}
Abraham I, Delling D, Goldberg AV, Werneck RF (2011) A hub-based labeling
  algorithm for shortest paths in road networks. \emph{International Symposium
  on Experimental Algorithms}.

\bibitem[{Abraham et~al.(2010)Abraham, Fiat, Goldberg, \protect\BIBand{}
  Werneck}]{highway2010}
Abraham I, Fiat A, Goldberg AV, Werneck RF (2010) Highway dimension, shortest
  paths, and provably efficient algorithms. \emph{Proceedings of the
  twenty-first annual ACM-SIAM symposium on Discrete Algorithms}.

\bibitem[{Babenko et~al.(2015)Babenko, Goldberg, Kaplan, Savchenko,
  \protect\BIBand{} Weller}]{babenko_hl_complexity}
Babenko M, Goldberg AV, Kaplan H, Savchenko R, Weller M (2015) On the
  complexity of hub labeling. \emph{International Symposium on Mathematical
  Foundations of Computer Science}.

\bibitem[{Barrett et~al.(2008)Barrett, Bisset, Holzer, Konjevod, Marathe,
  \protect\BIBand{} Wagner}]{language_csp}
Barrett C, Bisset K, Holzer M, Konjevod G, Marathe M, Wagner D (2008)
  Engineering label-constrained shortest-path algorithms. \emph{International
  Conference on Algorithmic Applications in Management}.

\bibitem[{Bast et~al.(2016)Bast, Delling, Goldberg, Pajor,
  M{\"u}ller-Hannemann, Sanders, Wagner, \protect\BIBand{}
  Werneck}]{goldberg_survey}
Bast H, Delling D, Goldberg A, Pajor T, M{\"u}ller-Hannemann M, Sanders P,
  Wagner D, Werneck RF (2016) Route planning in transportation networks.
  \emph{Algorithm Engineering}.

\bibitem[{Clawson et~al.(2015)Clawson, Ding, Englot, Frewen, Sisson,
  \protect\BIBand{} Vladimirsky}]{alex_bicriteria}
Clawson Z, Ding X, Englot B, Frewen TA, Sisson WM, Vladimirsky A (2015) A
  bi-criteria path planning algorithm for robotics applications. \emph{arXiv
  preprint arXiv:1511.01166} .

\bibitem[{Cohen et~al.(2003)Cohen, Halperin, Kaplan, \protect\BIBand{}
  Zwick}]{cohen_definition_hl}
Cohen E, Halperin E, Kaplan H, Zwick U (2003) Reachability and distance queries
  via 2-hop labels. \emph{SIAM Journal on Computing} 32(5).

\bibitem[{Correa et~al.(2017)Correa, Harks, Kreuzen, \protect\BIBand{}
  Matuschke}]{fareevasion}
Correa J, Harks T, Kreuzen VJC, Matuschke J (2017) Fare evasion in transit
  networks. \emph{Operations Research} 65(1):165--183.

\bibitem[{Demetrescu et~al.(2009)Demetrescu, Goldberg, \protect\BIBand{}
  Johnson}]{dimacs09}
Demetrescu C, Goldberg AV, Johnson DS (2009) \emph{The Shortest Path Problem:
  Ninth DIMACS Implementation Challenge}, volume~74.

\bibitem[{Even et~al.(2005)Even, Rawitz, \protect\BIBand{}
  Shahar}]{vc_dim_hitting}
Even G, Rawitz D, Shahar SM (2005) Hitting sets when the vc-dimension is small.
  \emph{Information Processing Letters} 95(2).

\bibitem[{Fan et~al.(2005)Fan, Kalaba, \protect\BIBand{}
  Moore~II}]{fan2005arriving}
Fan Y, Kalaba R, Moore~II J (2005) Arriving on time. \emph{Journal of
  Optimization Theory and Applications} 127(3).

\bibitem[{Festa(2015)}]{csp_survey}
Festa P (2015) Constrained shortest path problems: state-of-the-art and recent
  advances. \emph{Transparent Optical Networks (ICTON), 2015 17th International
  Conference on}.

\bibitem[{Geisberger et~al.(2008)Geisberger, Sanders, Schultes,
  \protect\BIBand{} Delling}]{geisberger_ch_definition}
Geisberger R, Sanders P, Schultes D, Delling D (2008) Contraction hierarchies:
  Faster and simpler hierarchical routing in road networks. \emph{International
  Workshop on Experimental and Efficient Algorithms}.

\bibitem[{Hoy \protect\BIBand{} Nikolova(2015)}]{nikolova_discretization}
Hoy D, Nikolova E (2015) Approximately optimal risk-averse routing policies via
  adaptive discretization. \emph{AAAI}.

\bibitem[{Hunter et~al.(2013)Hunter, Abbeel, \protect\BIBand{} Bayen}]{sf_data}
Hunter T, Abbeel P, Bayen AM (2013) The path inference filter: model-based
  low-latency map matching of probe vehicle data. \emph{Algorithmic Foundations
  of Robotics X}.

\bibitem[{Kosowski \protect\BIBand{} Viennot(2017)}]{skeleton}
Kosowski A, Viennot L (2017) Beyond highway dimension: Small distance labels
  using tree skeletons. \emph{Proceedings of the Twenty-Eighth Annual ACM-SIAM
  Symposium on Discrete Algorithms}.

\bibitem[{Niknami \protect\BIBand{} Samaranayake(2016)}]{niknami2016tractable}
Niknami M, Samaranayake S (2016) Tractable pathfinding for the stochastic
  on-time arrival problem. \emph{International Symposium on Experimental
  Algorithms}.

\bibitem[{Nikolova et~al.(2006)Nikolova, Kelner, Brand, \protect\BIBand{}
  Mitzenmacher}]{nikolova_gaussian}
Nikolova E, Kelner JA, Brand M, Mitzenmacher M (2006) Stochastic shortest paths
  via quasi-convex maximization. \emph{European Symposium on Algorithms}.

\bibitem[{Rice \protect\BIBand{} Tsotras(2010)}]{rice_csp}
Rice M, Tsotras VJ (2010) Graph indexing of road networks for shortest path
  queries with label restrictions. \emph{Proceedings of the VLDB Endowment}
  4(2).

\bibitem[{Sabran et~al.(2014)Sabran, Samaranayake, \protect\BIBand{}
  Bayen}]{sabran2014precomputation}
Sabran G, Samaranayake S, Bayen A (2014) Precomputation techniques for the
  stochastic on-time arrival problem. \emph{2014 Proceedings of the Sixteenth
  Workshop on Algorithm Engineering and Experiments (ALENEX)}.

\bibitem[{White(2015)}]{white_complexity_hd}
White C (2015) Lower bounds in the preprocessing and query phases of routing
  algorithms. \emph{Algorithms-ESA 2015}.

\bibitem[{Woodard et~al.(2017)Woodard, Nogin, Koch, Racz, Goldszmidt,
  \protect\BIBand{} Horvitz}]{woodard2017predicting}
Woodard D, Nogin G, Koch P, Racz D, Goldszmidt M, Horvitz E (2017) Predicting
  travel time reliability using mobile phone gps data. \emph{Transportation
  Research Part C: Emerging Technologies} 75:30--44.

\end{thebibliography}
% if more than one, comma separated


\end{document}


